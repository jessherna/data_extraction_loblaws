{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04494130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: selenium in c:\\users\\jessi\\appdata\\roaming\\python\\python312\\site-packages (4.39.0)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\jessi\\appdata\\roaming\\python\\python312\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.6.1)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in c:\\users\\jessi\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\jessi\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in c:\\users\\jessi\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (2025.11.12)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in c:\\users\\jessi\\appdata\\roaming\\python\\python312\\site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\jessi\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\jessi\\appdata\\roaming\\python\\python312\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\jessi\\appdata\\roaming\\python\\python312\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.31.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in c:\\users\\jessi\\appdata\\roaming\\python\\python312\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install -U selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce50cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "ROOT_URL = \"https://www.loblaws.ca/en\"\n",
    "MAX_WAIT = 60\n",
    "TARGET_CATEGORIES = [\"GROCERY\", \"HOME, BEAUTY & BABY\"]\n",
    "\n",
    "\n",
    "def get_driver(headless=False):\n",
    "    \"\"\"Create Chrome WebDriver.\"\"\"\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--start-maximized\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    return webdriver.Chrome(options=opts)\n",
    "\n",
    "\n",
    "def dismiss_overlays(driver):\n",
    "    \"\"\"Close popups.\"\"\"\n",
    "    for selector in ['[data-testid=\"close-button\"]', 'button[aria-label*=\"close\" i]']:\n",
    "        try:\n",
    "            for el in driver.find_elements(By.CSS_SELECTOR, selector):\n",
    "                if el.is_displayed():\n",
    "                    el.click()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087f456",
   "metadata": {},
   "source": [
    "## Extraction Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a297ee",
   "metadata": {},
   "source": [
    "### Extracting main categories from the menu bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "719b3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_main_categories(driver, wait):\n",
    "    \"\"\"Extract main categories from navigation with aria-controls.\"\"\"\n",
    "    if driver.current_url != ROOT_URL:\n",
    "        driver.get(ROOT_URL)\n",
    "        time.sleep(2)\n",
    "        dismiss_overlays(driver)\n",
    "    \n",
    "    # Wait for nav and then find l1_div using a direct path\n",
    "    nav = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'nav[data-testid=\"iceberg-menu-bar\"]')))\n",
    "    \n",
    "    # Try direct CSS path\n",
    "    try:\n",
    "        l1_div = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \n",
    "            'nav[data-testid=\"iceberg-menu-bar\"] > div > div[data-testid=\"iceberg-masthead-left\"] > div[data-testid=\"iceberg-main-nav-l1\"]')))\n",
    "    except:\n",
    "        # Fallback: navigate step by step\n",
    "        first_div = nav.find_element(By.CSS_SELECTOR, 'div')\n",
    "        masthead_left = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-testid=\"iceberg-masthead-left\"]')))\n",
    "        l1_div = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-testid=\"iceberg-main-nav-l1\"]')))\n",
    "    \n",
    "    ul = l1_div.find_element(By.CSS_SELECTOR, 'ul')\n",
    "    li_elements = ul.find_elements(By.CSS_SELECTOR, 'li')\n",
    "    \n",
    "    categories = []\n",
    "    for li in li_elements:\n",
    "        try:\n",
    "            btn = li.find_element(By.CSS_SELECTOR, 'button[data-testid=\"iceberg-main-nav-l1-button\"], a[data-testid=\"iceberg-main-nav-l1-button\"]')\n",
    "            span = btn.find_element(By.CSS_SELECTOR, 'span')\n",
    "            text = span.text.strip()\n",
    "            aria_controls = btn.get_attribute('aria-controls')\n",
    "            \n",
    "            if text:\n",
    "                # Check if category already exists\n",
    "                existing = next((c for c in categories if c['name'] == text), None)\n",
    "                if not existing:\n",
    "                    categories.append({\"name\": text, \"aria-controls\": aria_controls})\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208af9f5",
   "metadata": {},
   "source": [
    "### Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f90428ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing extract_main_categories()...\n",
      "\n",
      "✓ Extracted 7 categories:\n",
      "  1. GROCERY (aria-controls: radix-:r14:)\n",
      "  2. HOME, BEAUTY & BABY (aria-controls: radix-:r15:)\n",
      "  3. NEW (aria-controls: radix-:r16:)\n",
      "  4. FLYERS & DEALS (aria-controls: radix-:r17:)\n",
      "  5. PHARMACY & SERVICES (aria-controls: radix-:r18:)\n",
      "  6. MY SHOP (aria-controls: None)\n",
      "  7. PC EXPRESS PASS (aria-controls: None)\n",
      "\n",
      "✓ Test passed! Found 7 categories.\n"
     ]
    }
   ],
   "source": [
    "# Test extract_main_categories\n",
    "driver = get_driver(headless=False)\n",
    "wait = WebDriverWait(driver, MAX_WAIT)\n",
    "\n",
    "try:\n",
    "    print(\"Testing extract_main_categories()...\")\n",
    "    categories = extract_main_categories(driver, wait)\n",
    "    \n",
    "    print(f\"\\n✓ Extracted {len(categories)} categories:\")\n",
    "    for i, cat in enumerate(categories, 1):\n",
    "        print(f\"  {i}. {cat['name']} (aria-controls: {cat.get('aria-controls', 'N/A')})\")\n",
    "    \n",
    "    print(f\"\\n✓ Test passed! Found {len(categories)} categories.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d77bd",
   "metadata": {},
   "source": [
    "### Extracting Sub Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc10b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subcategories(driver, wait, category_name, aria_controls=None):\n",
    "    \"\"\"Extract subcategories for a category using aria-controls.\"\"\"\n",
    "    if category_name.upper() not in [cat.upper() for cat in TARGET_CATEGORIES]:\n",
    "        return []\n",
    "    \n",
    "    if aria_controls is None:\n",
    "        return []\n",
    "    \n",
    "    if driver.current_url != ROOT_URL:\n",
    "        driver.get(ROOT_URL)\n",
    "        time.sleep(2)\n",
    "        dismiss_overlays(driver)\n",
    "    \n",
    "    # Find the category button to hover over\n",
    "    nav = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'nav[data-testid=\"iceberg-menu-bar\"]')))\n",
    "    try:\n",
    "        l1_div = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \n",
    "            'nav[data-testid=\"iceberg-menu-bar\"] > div > div[data-testid=\"iceberg-masthead-left\"] > div[data-testid=\"iceberg-main-nav-l1\"]')))\n",
    "    except:\n",
    "        first_div = nav.find_element(By.CSS_SELECTOR, 'div')\n",
    "        masthead_left = first_div.find_element(By.CSS_SELECTOR, 'div[data-testid=\"iceberg-masthead-left\"]')\n",
    "        l1_div = masthead_left.find_element(By.CSS_SELECTOR, 'div[data-testid=\"iceberg-main-nav-l1\"]')\n",
    "    \n",
    "    # Find the category button by aria-controls (use XPath to handle colons in ID)\n",
    "    cat_btn = l1_div.find_element(By.XPATH, f'.//button[@aria-controls=\"{aria_controls}\"] | .//a[@aria-controls=\"{aria_controls}\"]')\n",
    "    \n",
    "    # Try to click the chevron/arrow to open the dropdown\n",
    "    try:\n",
    "        chevron = cat_btn.find_element(By.CSS_SELECTOR, 'svg[data-testid=\"iceberg-main-nav-l1-button-chevron\"]')\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", cat_btn)\n",
    "        time.sleep(0.5)\n",
    "        chevron.click()\n",
    "    except:\n",
    "        # Fallback: click the button itself or hover\n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", cat_btn)\n",
    "            time.sleep(0.5)\n",
    "            cat_btn.click()\n",
    "        except:\n",
    "            # Last resort: hover\n",
    "            ActionChains(driver).move_to_element(cat_btn).perform()\n",
    "    \n",
    "    time.sleep(1.5)\n",
    "    \n",
    "    # Find the popover content using the aria-controls id (use XPath to handle colons)\n",
    "    popover_content = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"{aria_controls}\"]')))\n",
    "    \n",
    "    # Navigate to the tablist inside the popover\n",
    "    tabs_div = popover_content.find_element(By.CSS_SELECTOR, 'div[data-testid=\"iceberg-main-nav-l2-tabs\"]')\n",
    "    tablist = tabs_div.find_element(By.CSS_SELECTOR, 'div[role=\"tablist\"][data-testid=\"iceberg-main-nav-l2-tabs-list\"]')\n",
    "    buttons = tablist.find_elements(By.CSS_SELECTOR, 'button[data-testid=\"iceberg-main-nav-l2-button\"], a[data-testid=\"iceberg-main-nav-l2-button\"]')\n",
    "    \n",
    "    subcategories = []\n",
    "    for btn in buttons:\n",
    "        try:\n",
    "            # Find span inside a element\n",
    "            a_elem = btn.find_element(By.CSS_SELECTOR, 'a')\n",
    "            span = a_elem.find_element(By.CSS_SELECTOR, 'span')\n",
    "            text = span.text.strip()\n",
    "            \n",
    "            if text and text not in [s['name'] for s in subcategories]:\n",
    "                subcategories.append({\"name\": text})\n",
    "        except:\n",
    "            # Fallback: try direct span (for links that are already <a> elements)\n",
    "            try:\n",
    "                span = btn.find_element(By.CSS_SELECTOR, 'span')\n",
    "                text = span.text.strip()\n",
    "                \n",
    "                if text and text not in [s['name'] for s in subcategories]:\n",
    "                    subcategories.append({\"name\": text})\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    return subcategories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43428f5",
   "metadata": {},
   "source": [
    "### Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f5df3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing extract_subcategories()...\n",
      "Extracting subcategories for: GROCERY\n",
      "Using aria-controls: radix-:r13:\n",
      "\n",
      "✓ Extracted 15 subcategories:\n",
      "  1. Fruits & Vegetables\n",
      "  2. Dairy & Eggs\n",
      "  3. Meat\n",
      "  4. Pantry\n",
      "  5. International Foods\n",
      "  6. Snacks, Chips & Candy\n",
      "  7. Frozen Food\n",
      "  8. Natural and Organic\n",
      "  9. Bakery\n",
      "  10. Prepared Meals\n",
      "  11. Drinks\n",
      "  12. Deli\n",
      "  13. Fish & Seafood\n",
      "  14. Beer & Wine\n",
      "  15. Floral Shop\n",
      "\n",
      "✓ Test passed! Found 15 subcategories for GROCERY.\n"
     ]
    }
   ],
   "source": [
    "# Test extract_subcategories\n",
    "driver = get_driver(headless=False)\n",
    "wait = WebDriverWait(driver, MAX_WAIT)\n",
    "\n",
    "try:\n",
    "    print(\"Testing extract_subcategories()...\")\n",
    "    \n",
    "    # First get categories with aria-controls\n",
    "    categories = extract_main_categories(driver, wait)\n",
    "    \n",
    "    # Test with GROCERY category (one of the target categories)\n",
    "    category_name = \"GROCERY\"\n",
    "    print(f\"Extracting subcategories for: {category_name}\")\n",
    "    \n",
    "    # Find the category's aria-controls\n",
    "    category_data = next((c for c in categories if category_name.upper() in c['name'].upper()), None)\n",
    "    if not category_data or not category_data.get('aria-controls'):\n",
    "        print(f\"\\n✗ Could not find {category_name} or aria-controls\")\n",
    "    else:\n",
    "        aria_controls = category_data['aria-controls']\n",
    "        print(f\"Using aria-controls: {aria_controls}\")\n",
    "        \n",
    "        subcategories = extract_subcategories(driver, wait, category_name, aria_controls)\n",
    "        \n",
    "        print(f\"\\n✓ Extracted {len(subcategories)} subcategories:\")\n",
    "        for i, subcat in enumerate(subcategories, 1):\n",
    "            print(f\"  {i}. {subcat['name']}\")\n",
    "        \n",
    "        if len(subcategories) > 0:\n",
    "            print(f\"\\n✓ Test passed! Found {len(subcategories)} subcategories for {category_name}.\")\n",
    "        else:\n",
    "            print(f\"\\n⚠ Test completed but no subcategories found. This might be normal if the page structure is different.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10017e2e",
   "metadata": {},
   "source": [
    "### Extracting subcategory2 from dropdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc1d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subcategory2_from_dropdown(driver, wait, category_name, subcategory_name, category_aria_controls, popover_content=None):\n",
    "    \"\"\"\n",
    "    Extract subcategory2 items from dropdown by clicking on subcategory button.\n",
    "    \n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance\n",
    "        wait: WebDriverWait instance\n",
    "        category_name: Name of the category\n",
    "        subcategory_name: Name of the subcategory\n",
    "        category_aria_controls: aria-controls value for the category\n",
    "        popover_content: Optional pre-opened popover content element (for optimization)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of subcategory2 items with name and url\n",
    "    \"\"\"\n",
    "    # Only navigate if not already on ROOT_URL or if popover_content not provided\n",
    "    if popover_content is None:\n",
    "        if driver.current_url != ROOT_URL:\n",
    "            driver.get(ROOT_URL)\n",
    "            time.sleep(1.5)  # Reduced from 2\n",
    "            dismiss_overlays(driver)\n",
    "    \n",
    "        # Find the category button and open dropdown\n",
    "        nav = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'nav[data-testid=\"iceberg-menu-bar\"]')))\n",
    "        try:\n",
    "            l1_div = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \n",
    "                'nav[data-testid=\"iceberg-menu-bar\"] > div > div[data-testid=\"iceberg-masthead-left\"] > div[data-testid=\"iceberg-main-nav-l1\"]')))\n",
    "        except:\n",
    "            first_div = nav.find_element(By.CSS_SELECTOR, 'div')\n",
    "            masthead_left = first_div.find_element(By.CSS_SELECTOR, 'div[data-testid=\"iceberg-masthead-left\"]')\n",
    "            l1_div = masthead_left.find_element(By.CSS_SELECTOR, 'div[data-testid=\"iceberg-main-nav-l1\"]')\n",
    "        \n",
    "        # Find and click the category button\n",
    "        cat_btn = l1_div.find_element(By.XPATH, f'.//button[@aria-controls=\"{category_aria_controls}\"] | .//a[@aria-controls=\"{category_aria_controls}\"]')\n",
    "        \n",
    "        try:\n",
    "            chevron = cat_btn.find_element(By.CSS_SELECTOR, 'svg[data-testid=\"iceberg-main-nav-l1-button-chevron\"]')\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", cat_btn)\n",
    "            time.sleep(0.3)  # Reduced from 0.5\n",
    "            chevron.click()\n",
    "        except:\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", cat_btn)\n",
    "                time.sleep(0.3)  # Reduced from 0.5\n",
    "                cat_btn.click()\n",
    "            except:\n",
    "                ActionChains(driver).move_to_element(cat_btn).perform()\n",
    "    \n",
    "        time.sleep(1.2)  # Reduced from 1.5\n",
    "        \n",
    "        # Find the popover content\n",
    "        popover_content = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"{category_aria_controls}\"]')))\n",
    "    else:\n",
    "        # Popover already open, just need to find subcategory button\n",
    "        pass\n",
    "    \n",
    "    # Find the subcategory button\n",
    "    tabs_div = popover_content.find_element(By.CSS_SELECTOR, 'div[data-testid=\"iceberg-main-nav-l2-tabs\"]')\n",
    "    tablist = tabs_div.find_element(By.CSS_SELECTOR, 'div[role=\"tablist\"][data-testid=\"iceberg-main-nav-l2-tabs-list\"]')\n",
    "    buttons = tablist.find_elements(By.CSS_SELECTOR, 'button[data-testid=\"iceberg-main-nav-l2-button\"], a[data-testid=\"iceberg-main-nav-l2-button\"]')\n",
    "    \n",
    "    subcategory_btn = None\n",
    "    for btn in buttons:\n",
    "        try:\n",
    "            a_elem = btn.find_element(By.CSS_SELECTOR, 'a')\n",
    "            span = a_elem.find_element(By.CSS_SELECTOR, 'span')\n",
    "            if subcategory_name.upper() in span.text.upper():\n",
    "                subcategory_btn = btn\n",
    "                break\n",
    "        except:\n",
    "            try:\n",
    "                span = btn.find_element(By.CSS_SELECTOR, 'span')\n",
    "                if subcategory_name.upper() in span.text.upper():\n",
    "                    subcategory_btn = btn\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if subcategory_btn is None:\n",
    "        return []\n",
    "    \n",
    "    # Click the subcategory button to show its content\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", subcategory_btn)\n",
    "    time.sleep(0.1)  # Reduced from 0.2\n",
    "    subcategory_btn.click()\n",
    "    time.sleep(0.3)  # Reduced from 0.5\n",
    "    \n",
    "    # Get the button's aria-controls to find the content panel\n",
    "    btn_id = subcategory_btn.get_attribute('id')\n",
    "    btn_aria_controls = subcategory_btn.get_attribute('aria-controls')\n",
    "    \n",
    "    if not btn_aria_controls and btn_id and '-trigger-' in btn_id:\n",
    "        btn_aria_controls = btn_id.replace('-trigger-', '-content-')\n",
    "    \n",
    "    if not btn_aria_controls:\n",
    "        return []\n",
    "    \n",
    "    # Find the content panel and extract subcategory2 items\n",
    "    try:\n",
    "        content_panel = popover_content.find_element(By.XPATH, f'.//*[@id=\"{btn_aria_controls}\"]')\n",
    "        content_list = content_panel.find_element(By.CSS_SELECTOR, 'ul[data-testid=\"iceberg-main-nav-l3-content-list\"]')\n",
    "        links = content_list.find_elements(By.CSS_SELECTOR, 'a[data-testid=\"iceberg-main-nav-l3-button\"]')\n",
    "        \n",
    "        subcategory2_list = []\n",
    "        for link in links:\n",
    "            try:\n",
    "                link_text = link.text.strip()\n",
    "                link_href = link.get_attribute('href')\n",
    "                # Skip \"See All\" links\n",
    "                if link_text and link_href and not link_text.lower().startswith('see all'):\n",
    "                    if not link_href.startswith('http'):\n",
    "                        link_href = urljoin(ROOT_URL, link_href)\n",
    "                    subcategory2_list.append({\n",
    "                        \"name\": link_text,\n",
    "                        \"url\": link_href\n",
    "                    })\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return subcategory2_list\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3785c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subcategory2_while_dropdown_open(driver, wait, category_name, subcategory_name, popover_content, all_subcategory_buttons):\n",
    "    \"\"\"\n",
    "    Extract subcategory2 items for a specific subcategory while dropdown is already open.\n",
    "    This is optimized for sequential processing within the same category.\n",
    "    \n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance\n",
    "        wait: WebDriverWait instance\n",
    "        category_name: Name of the category\n",
    "        subcategory_name: Name of the subcategory\n",
    "        popover_content: Already opened popover content element\n",
    "        all_subcategory_buttons: List of all subcategory buttons (pre-fetched)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of subcategory2 items with name and url\n",
    "    \"\"\"\n",
    "    subcategory2_list = []\n",
    "    \n",
    "    # Find the matching subcategory button\n",
    "    subcategory_btn = None\n",
    "    for btn in all_subcategory_buttons:\n",
    "        try:\n",
    "            a_elem = btn.find_element(By.CSS_SELECTOR, 'a')\n",
    "            span = a_elem.find_element(By.CSS_SELECTOR, 'span')\n",
    "            if subcategory_name.upper() in span.text.upper():\n",
    "                subcategory_btn = btn\n",
    "                break\n",
    "        except:\n",
    "            try:\n",
    "                span = btn.find_element(By.CSS_SELECTOR, 'span')\n",
    "                if subcategory_name.upper() in span.text.upper():\n",
    "                    subcategory_btn = btn\n",
    "                    break\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    if subcategory_btn is None:\n",
    "        return []\n",
    "    \n",
    "    # Click the subcategory button to show its content\n",
    "    try:\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", subcategory_btn)\n",
    "        time.sleep(0.1)\n",
    "        subcategory_btn.click()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        # Get the button's aria-controls to find the content panel\n",
    "        btn_id = subcategory_btn.get_attribute('id')\n",
    "        btn_aria_controls = subcategory_btn.get_attribute('aria-controls')\n",
    "        \n",
    "        if not btn_aria_controls and btn_id and '-trigger-' in btn_id:\n",
    "            btn_aria_controls = btn_id.replace('-trigger-', '-content-')\n",
    "        \n",
    "        if not btn_aria_controls:\n",
    "            return []\n",
    "        \n",
    "        # Find the content panel and extract subcategory2 items\n",
    "        try:\n",
    "            content_panel = popover_content.find_element(By.XPATH, f'.//*[@id=\"{btn_aria_controls}\"]')\n",
    "            content_list = content_panel.find_element(By.CSS_SELECTOR, 'ul[data-testid=\"iceberg-main-nav-l3-content-list\"]')\n",
    "            links = content_list.find_elements(By.CSS_SELECTOR, 'a[data-testid=\"iceberg-main-nav-l3-button\"]')\n",
    "            \n",
    "            for link in links:\n",
    "                try:\n",
    "                    link_text = link.text.strip()\n",
    "                    link_href = link.get_attribute('href')\n",
    "                    # Skip \"See All\" links\n",
    "                    if link_text and link_href and not link_text.lower().startswith('see all'):\n",
    "                        if not link_href.startswith('http'):\n",
    "                            link_href = urljoin(ROOT_URL, link_href)\n",
    "                        subcategory2_list.append({\n",
    "                            \"name\": link_text,\n",
    "                            \"url\": link_href\n",
    "                        })\n",
    "                except:\n",
    "                    continue\n",
    "        except:\n",
    "            pass\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return subcategory2_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "463f6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_subcategory2_for_category(driver, wait, category_name, subcategories, category_aria_controls):\n",
    "    \"\"\"\n",
    "    Optimized: Extract all subcategory2 items for all subcategories in one dropdown session.\n",
    "    This avoids repeatedly opening/closing the dropdown.\n",
    "    \n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance\n",
    "        wait: WebDriverWait instance\n",
    "        category_name: Name of the category\n",
    "        subcategories: List of subcategory dicts with 'name' key\n",
    "        category_aria_controls: aria-controls value for the category\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping of subcategory_name -> list of subcategory2 items\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    # Navigate to ROOT_URL and open dropdown once\n",
    "    if driver.current_url != ROOT_URL:\n",
    "        driver.get(ROOT_URL)\n",
    "        time.sleep(1.5)\n",
    "        dismiss_overlays(driver)\n",
    "    \n",
    "    # Find the category button and open dropdown\n",
    "    nav = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'nav[data-testid=\"iceberg-menu-bar\"]')))\n",
    "    try:\n",
    "        l1_div = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \n",
    "            'nav[data-testid=\"iceberg-menu-bar\"] > div > div[data-testid=\"iceberg-masthead-left\"] > div[data-testid=\"iceberg-main-nav-l1\"]')))\n",
    "    except:\n",
    "        first_div = nav.find_element(By.CSS_SELECTOR, 'div')\n",
    "        masthead_left = first_div.find_element(By.CSS_SELECTOR, 'div[data-testid=\"iceberg-masthead-left\"]')\n",
    "        l1_div = masthead_left.find_element(By.CSS_SELECTOR, 'div[data-testid=\"iceberg-main-nav-l1\"]')\n",
    "    \n",
    "    # Find and click the category button\n",
    "    cat_btn = l1_div.find_element(By.XPATH, f'.//button[@aria-controls=\"{category_aria_controls}\"] | .//a[@aria-controls=\"{category_aria_controls}\"]')\n",
    "    \n",
    "    try:\n",
    "        chevron = cat_btn.find_element(By.CSS_SELECTOR, 'svg[data-testid=\"iceberg-main-nav-l1-button-chevron\"]')\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", cat_btn)\n",
    "        time.sleep(0.3)\n",
    "        chevron.click()\n",
    "    except:\n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", cat_btn)\n",
    "            time.sleep(0.3)\n",
    "            cat_btn.click()\n",
    "        except:\n",
    "            ActionChains(driver).move_to_element(cat_btn).perform()\n",
    "    \n",
    "    time.sleep(1.2)\n",
    "    \n",
    "    # Get the popover content once\n",
    "    popover_content = wait.until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"{category_aria_controls}\"]')))\n",
    "    \n",
    "    # Get all subcategory buttons\n",
    "    tabs_div = popover_content.find_element(By.CSS_SELECTOR, 'div[data-testid=\"iceberg-main-nav-l2-tabs\"]')\n",
    "    tablist = tabs_div.find_element(By.CSS_SELECTOR, 'div[role=\"tablist\"][data-testid=\"iceberg-main-nav-l2-tabs-list\"]')\n",
    "    all_subcategory_buttons = tablist.find_elements(By.CSS_SELECTOR, 'button[data-testid=\"iceberg-main-nav-l2-button\"], a[data-testid=\"iceberg-main-nav-l2-button\"]')\n",
    "    \n",
    "    # Process each subcategory\n",
    "    for subcategory in subcategories:\n",
    "        subcategory_name = subcategory['name']\n",
    "        result[subcategory_name] = []\n",
    "        \n",
    "        # Find the matching subcategory button\n",
    "        subcategory_btn = None\n",
    "        for btn in all_subcategory_buttons:\n",
    "            try:\n",
    "                a_elem = btn.find_element(By.CSS_SELECTOR, 'a')\n",
    "                span = a_elem.find_element(By.CSS_SELECTOR, 'span')\n",
    "                if subcategory_name.upper() in span.text.upper():\n",
    "                    subcategory_btn = btn\n",
    "                    break\n",
    "            except:\n",
    "                try:\n",
    "                    span = btn.find_element(By.CSS_SELECTOR, 'span')\n",
    "                    if subcategory_name.upper() in span.text.upper():\n",
    "                        subcategory_btn = btn\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        if subcategory_btn is None:\n",
    "            continue\n",
    "        \n",
    "        # Click the subcategory button to show its content\n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", subcategory_btn)\n",
    "            time.sleep(0.1)\n",
    "            subcategory_btn.click()\n",
    "            time.sleep(0.3)\n",
    "            \n",
    "            # Get the button's aria-controls to find the content panel\n",
    "            btn_id = subcategory_btn.get_attribute('id')\n",
    "            btn_aria_controls = subcategory_btn.get_attribute('aria-controls')\n",
    "            \n",
    "            if not btn_aria_controls and btn_id and '-trigger-' in btn_id:\n",
    "                btn_aria_controls = btn_id.replace('-trigger-', '-content-')\n",
    "            \n",
    "            if not btn_aria_controls:\n",
    "                continue\n",
    "            \n",
    "            # Find the content panel and extract subcategory2 items\n",
    "            try:\n",
    "                content_panel = popover_content.find_element(By.XPATH, f'.//*[@id=\"{btn_aria_controls}\"]')\n",
    "                content_list = content_panel.find_element(By.CSS_SELECTOR, 'ul[data-testid=\"iceberg-main-nav-l3-content-list\"]')\n",
    "                links = content_list.find_elements(By.CSS_SELECTOR, 'a[data-testid=\"iceberg-main-nav-l3-button\"]')\n",
    "                \n",
    "                for link in links:\n",
    "                    try:\n",
    "                        link_text = link.text.strip()\n",
    "                        link_href = link.get_attribute('href')\n",
    "                        # Skip \"See All\" links\n",
    "                        if link_text and link_href and not link_text.lower().startswith('see all'):\n",
    "                            if not link_href.startswith('http'):\n",
    "                                link_href = urljoin(ROOT_URL, link_href)\n",
    "                            result[subcategory_name].append({\n",
    "                                \"name\": link_text,\n",
    "                                \"url\": link_href\n",
    "                            })\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7c5d766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing extract_subcategory2_from_dropdown()...\n",
      "Extracting subcategory2 for: GROCERY > Dairy & Eggs\n",
      "Using aria-controls: radix-:r1e:\n",
      "\n",
      "✓ Extracted 9 subcategory2 items:\n",
      "  1. Milk & Cream\n",
      "      url: https://www.loblaws.ca/en/food/dairy-eggs/milk-cream/c/28224?navid=flyout-L3-Milk-and-Cream\n",
      "  2. Egg & Egg Substitutes\n",
      "      url: https://www.loblaws.ca/en/food/dairy-eggs/egg-egg-substitutes/c/28222?navid=flyout-L3-Egg-and-Egg-Substitutes\n",
      "  3. Butter & Spreads\n",
      "      url: https://www.loblaws.ca/en/food/dairy-eggs/butter-spreads/c/28220?navid=flyout-L3-Butter-and-Spreads\n",
      "  4. Cheese\n",
      "      url: https://www.loblaws.ca/en/food/dairy-eggs/cheese/c/28225?navid=flyout-L3-Cheese\n",
      "  5. Yogurt\n",
      "      url: https://www.loblaws.ca/en/food/dairy-eggs/yogurt/c/28227?navid=flyout-L3-Yogurt\n",
      "  6. Desserts & Doughs\n",
      "      url: https://www.loblaws.ca/en/food/dairy-eggs/desserts-doughs/c/28221?navid=flyout-L3-Desserts-and-Doughs\n",
      "  7. Sour Cream & Dips\n",
      "      url: https://www.loblaws.ca/en/food/dairy-eggs/sour-cream-dips/c/28226?navid=flyout-L3-Sour-Cream-and-Dips\n",
      "  8. Lactose Free\n",
      "      url: https://www.loblaws.ca/en/food/dairy-eggs/lactose-free/c/28223?navid=flyout-L3-Lactose-Free\n",
      "  9. Non-Dairy Milk Alternatives\n",
      "      url: https://www.loblaws.ca/en/food/dairy-eggs/non-dairy-milk-alternatives/c/58904?navid=flyout-L3-Food-DairyandEggs-Non-DairyMilkAlternatives\n",
      "\n",
      "✓ Test passed! Found 9 subcategory2 items.\n"
     ]
    }
   ],
   "source": [
    "### Test extract_subcategory2_from_dropdown\n",
    "\n",
    "# Test extract_subcategory2_from_dropdown\n",
    "driver = get_driver(headless=False)\n",
    "wait = WebDriverWait(driver, MAX_WAIT)\n",
    "\n",
    "try:\n",
    "    print(\"Testing extract_subcategory2_from_dropdown()...\")\n",
    "    \n",
    "    # Get categories\n",
    "    categories = extract_main_categories(driver, wait)\n",
    "    category_name = \"GROCERY\"\n",
    "    subcategory_name = \"Dairy & Eggs\"\n",
    "    \n",
    "    category_data = next((c for c in categories if category_name.upper() in c['name'].upper()), None)\n",
    "    if not category_data or not category_data.get('aria-controls'):\n",
    "        print(f\"\\n✗ Could not find {category_name} or aria-controls\")\n",
    "    else:\n",
    "        aria_controls = category_data['aria-controls']\n",
    "        print(f\"Extracting subcategory2 for: {category_name} > {subcategory_name}\")\n",
    "        print(f\"Using aria-controls: {aria_controls}\")\n",
    "        \n",
    "        subcategory2_list = extract_subcategory2_from_dropdown(driver, wait, category_name, subcategory_name, aria_controls)\n",
    "        \n",
    "        print(f\"\\n✓ Extracted {len(subcategory2_list)} subcategory2 items:\")\n",
    "        for i, item in enumerate(subcategory2_list, 1):\n",
    "            print(f\"  {i}. {item['name']}\")\n",
    "            print(f\"      url: {item['url']}\")\n",
    "        \n",
    "        if len(subcategory2_list) > 0:\n",
    "            print(f\"\\n✓ Test passed! Found {len(subcategory2_list)} subcategory2 items.\")\n",
    "        else:\n",
    "            print(f\"\\n⚠ Test completed but no subcategory2 items found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Test failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38dd036",
   "metadata": {},
   "source": [
    "### Extract products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a8b16b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_products_from_grid(driver, wait, see_all_true_url, debug=False):\n",
    "    \"\"\"Extract products from product grid.\"\"\"\n",
    "    if debug:\n",
    "        print(f\"\\n[DEBUG] Navigating to: {see_all_true_url}\")\n",
    "    \n",
    "    driver.get(see_all_true_url)\n",
    "    time.sleep(2)  # Reduced from 3 - optimized for faster page loads\n",
    "    dismiss_overlays(driver)\n",
    "    \n",
    "    # Wait for the listing page container to appear\n",
    "    try:\n",
    "        listing_container = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid=\"listing-page-container\"]')))\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] ✓ Listing container found: [data-testid='listing-page-container']\")\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] ✗ Listing container NOT found: [data-testid='listing-page-container'] - {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Wait for the grid to appear within the listing container\n",
    "    try:\n",
    "        grid = listing_container.find_element(By.CSS_SELECTOR, '[data-testid=\"product-grid-component\"]')\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] ✓ Grid found: [data-testid='product-grid-component']\")\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] ✗ Grid NOT found: [data-testid='product-grid-component'] - {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Wait for at least one product title to appear (indicates products are loaded)\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'h3[data-testid=\"product-title\"]')))\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] ✓ At least one product title found\")\n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] ✗ No product titles found - {e}\")\n",
    "    \n",
    "    # Scroll multiple times to load all products (lazy loading)\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    scroll_attempts = 0\n",
    "    max_scrolls = 5\n",
    "    \n",
    "    while scroll_attempts < max_scrolls:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1.5)  # Reduced from 2 - optimized for faster product loading\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        scroll_attempts += 1\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"[DEBUG] Scrolled {scroll_attempts} times\")\n",
    "    \n",
    "    # Scroll back to top to ensure all elements are accessible\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    products = []\n",
    "    seen_titles = set()  # To avoid duplicates across pages\n",
    "    \n",
    "    # Ensure re module is available\n",
    "    try:\n",
    "        import re\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Check for pagination and find maximum page number\n",
    "    max_page = 1\n",
    "    try:\n",
    "        # Scroll down to find pagination (it might be at the bottom)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "        pagination = listing_container.find_element(By.CSS_SELECTOR, '[data-testid=\"pagination\"]')\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] ✓ Pagination container found\")\n",
    "        \n",
    "        # Get all page links with aria-label starting with \"Page\"\n",
    "        page_links = pagination.find_elements(By.CSS_SELECTOR, 'a[aria-label^=\"Page\"]')\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Found {len(page_links)} page links\")\n",
    "        \n",
    "        # Extract all page numbers from href attributes\n",
    "        page_numbers_found = []\n",
    "        for link in page_links:\n",
    "            try:\n",
    "                href = link.get_attribute('href')\n",
    "                aria_label = link.get_attribute('aria-label')\n",
    "                if debug and len(page_numbers_found) < 5:  # Debug first few\n",
    "                    print(f\"[DEBUG]   Link: aria-label='{aria_label}', href='{href}'\")\n",
    "                \n",
    "                if href:\n",
    "                    # Extract page number from ?page=N or &page=N\n",
    "                    try:\n",
    "                        match = re.search(r'[?&]page=(\\d+)', href)\n",
    "                        if match:\n",
    "                            page_num = int(match.group(1))\n",
    "                            if page_num not in page_numbers_found:\n",
    "                                page_numbers_found.append(page_num)\n",
    "                    except Exception as e:\n",
    "                        if debug:\n",
    "                            print(f\"[DEBUG]   Error extracting page number from '{href}': {e}\")\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG]   Error processing link: {e}\")\n",
    "        \n",
    "        if page_numbers_found:\n",
    "            max_page = max(page_numbers_found)\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] ✓ Found pagination - Maximum page: {max_page} (from visible pages: {sorted(page_numbers_found)})\")\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] ⚠ Found pagination but no page numbers extracted\")\n",
    "                # Try alternative: get all links in pagination\n",
    "                all_links = pagination.find_elements(By.CSS_SELECTOR, 'a')\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG]   Total links in pagination: {len(all_links)}\")\n",
    "                    for i, link in enumerate(all_links[:10]):\n",
    "                        print(f\"[DEBUG]   Link {i+1}: aria-label='{link.get_attribute('aria-label')}', href='{link.get_attribute('href')}'\")\n",
    "        \n",
    "        # Generate list of all page numbers from 1 to max_page\n",
    "        page_numbers = list(range(1, max_page + 1))\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Will process pages: {page_numbers}\")\n",
    "        \n",
    "        # Scroll back to top after finding pagination\n",
    "        driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"[DEBUG] ✗ No pagination found, extracting from single page: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        page_numbers = [1]\n",
    "    \n",
    "    # Extract products from each page\n",
    "    for page_num in page_numbers:\n",
    "        if page_num > 1:\n",
    "            # Build URL with page parameter\n",
    "            # Check if URL already has query parameters\n",
    "            if '?' in see_all_true_url:\n",
    "                page_url = f\"{see_all_true_url}&page={page_num}\"\n",
    "            else:\n",
    "                page_url = f\"{see_all_true_url}?page={page_num}\"\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Navigating to page {page_num}: {page_url}\")\n",
    "            \n",
    "            try:\n",
    "                driver.get(page_url)\n",
    "                time.sleep(2)  # Reduced from 3 - optimized for faster navigation\n",
    "                dismiss_overlays(driver)\n",
    "                # Re-find containers after navigation\n",
    "                listing_container = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid=\"listing-page-container\"]')))\n",
    "                grid = listing_container.find_element(By.CSS_SELECTOR, '[data-testid=\"product-grid-component\"]')\n",
    "                wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'h3[data-testid=\"product-title\"]')))\n",
    "                \n",
    "                # Scroll to load products\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(1.5)  # Reduced from 2\n",
    "                driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "                time.sleep(0.8)  # Reduced from 1\n",
    "                \n",
    "                if debug:\n",
    "                    print(f\"[DEBUG] ✓ Navigated to page {page_num}\")\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"[DEBUG] ✗ Could not navigate to page {page_num}: {e}\")\n",
    "                break\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"\\n[DEBUG] Extracting products from page {page_num}/{len(page_numbers)}\")\n",
    "        \n",
    "        # Structure: listing-page-container > product-grid-component > div.css-0 (multiple) > div.css-yyn1h > product content\n",
    "        # First find all div.css-0 elements within the grid\n",
    "        css0_containers = grid.find_elements(By.CSS_SELECTOR, 'div.css-0')\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Found {len(css0_containers)} containers with selector 'div.css-0' within grid\")\n",
    "        \n",
    "        # Now find div.css-yyn1h within each div.css-0\n",
    "        product_containers = []\n",
    "        for css0 in css0_containers:\n",
    "            try:\n",
    "                css_yyn1h = css0.find_element(By.CSS_SELECTOR, 'div.css-yyn1h')\n",
    "                product_containers.append(css_yyn1h)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Found {len(product_containers)} product containers (div.css-yyn1h) within div.css-0\")\n",
    "        \n",
    "        # If still no containers, try finding by product-title directly\n",
    "        if len(product_containers) == 0:\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] No containers found, trying to find by product titles...\")\n",
    "            product_titles = grid.find_elements(By.CSS_SELECTOR, 'h3[data-testid=\"product-title\"]')\n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Found {len(product_titles)} product titles directly\")\n",
    "            \n",
    "            for title in product_titles:\n",
    "                try:\n",
    "                    # Find the css-yyn1h ancestor\n",
    "                    parent = title.find_element(By.XPATH, './ancestor::div[contains(@class, \"css-yyn1h\")]')\n",
    "                    if parent not in product_containers:\n",
    "                        product_containers.append(parent)\n",
    "                except:\n",
    "                    try:\n",
    "                        # Fallback to chakra-linkbox\n",
    "                        parent = title.find_element(By.XPATH, './ancestor::div[contains(@class, \"chakra-linkbox\")]')\n",
    "                        if parent not in product_containers:\n",
    "                            product_containers.append(parent)\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"[DEBUG] Found {len(product_containers)} containers via title ancestors\")\n",
    "        \n",
    "        # Process containers for this page\n",
    "        for idx, container in enumerate(product_containers):\n",
    "            if debug and idx < 3:  # Debug first 3 containers per page\n",
    "                print(f\"\\n[DEBUG] Processing container {idx + 1}/{len(product_containers)} on page {page_num}\")\n",
    "            \n",
    "            try:\n",
    "                product_data = {}\n",
    "                \n",
    "                # Title - h3 with data-testid=\"product-title\" (inside the a.chakra-linkbox__overlay)\n",
    "                try:\n",
    "                    title = container.find_element(By.CSS_SELECTOR, 'h3[data-testid=\"product-title\"]')\n",
    "                    title_text = title.text.strip()\n",
    "                    if debug and idx < 3:\n",
    "                        print(f\"  [DEBUG] ✓ Title found: '{title_text[:50]}...'\")\n",
    "                    if not title_text or title_text in seen_titles:\n",
    "                        if debug and idx < 3:\n",
    "                            print(f\"  [DEBUG] ⚠ Skipping duplicate or empty title\")\n",
    "                        continue\n",
    "                    seen_titles.add(title_text)\n",
    "                    product_data[\"product-title\"] = title_text\n",
    "                except Exception as e:\n",
    "                    if debug and idx < 3:\n",
    "                        print(f\"  [DEBUG] ✗ Title NOT found: {e}\")\n",
    "                    continue  # Skip if no title found\n",
    "                \n",
    "                # Price - span with data-testid=\"regular-price\" or \"sale-price\" (inside div[data-testid=\"price-product-tile\"])\n",
    "                try:\n",
    "                    price_tile = container.find_element(By.CSS_SELECTOR, 'div[data-testid=\"price-product-tile\"]')\n",
    "                    if debug and idx < 3:\n",
    "                        print(f\"  [DEBUG] ✓ Price tile found\")\n",
    "                    # Try regular price first\n",
    "                    try:\n",
    "                        price = price_tile.find_element(By.CSS_SELECTOR, 'span[data-testid=\"regular-price\"]')\n",
    "                        price_text = price.text.strip()\n",
    "                        # Clean up price text (remove \"sale\" prefix if present)\n",
    "                        price_text = price_text.replace('sale', '').strip()\n",
    "                        product_data[\"regular-price\"] = price_text\n",
    "                        if debug and idx < 3:\n",
    "                            print(f\"  [DEBUG] ✓ Regular price found: '{price_text}'\")\n",
    "                    except:\n",
    "                        # Try sale price\n",
    "                        try:\n",
    "                            price = price_tile.find_element(By.CSS_SELECTOR, 'span[data-testid=\"sale-price\"]')\n",
    "                            price_text = price.text.strip()\n",
    "                            price_text = price_text.replace('sale', '').strip()\n",
    "                            product_data[\"regular-price\"] = price_text\n",
    "                            if debug and idx < 3:\n",
    "                                print(f\"  [DEBUG] ✓ Sale price found: '{price_text}'\")\n",
    "                        except:\n",
    "                            if debug and idx < 3:\n",
    "                                print(f\"  [DEBUG] ✗ No price span found in price tile\")\n",
    "                except Exception as e:\n",
    "                    if debug and idx < 3:\n",
    "                        print(f\"  [DEBUG] ✗ Price tile NOT found: {e}\")\n",
    "                \n",
    "                # Size - p with data-testid=\"product-package-size\" (inside the a.chakra-linkbox__overlay)\n",
    "                try:\n",
    "                    size = container.find_element(By.CSS_SELECTOR, 'p[data-testid=\"product-package-size\"]')\n",
    "                    product_data[\"product-package-size\"] = size.text.strip()\n",
    "                    if debug and idx < 3:\n",
    "                        print(f\"  [DEBUG] ✓ Size found: '{product_data['product-package-size']}'\")\n",
    "                except Exception as e:\n",
    "                    if debug and idx < 3:\n",
    "                        print(f\"  [DEBUG] ✗ Size NOT found: {e}\")\n",
    "                \n",
    "                # URL - try multiple selectors (prioritize div.css-qoklea > a)\n",
    "                href = None\n",
    "                # First try: a tag inside div.css-qoklea (as requested by user)\n",
    "                try:\n",
    "                    qoklea_div = container.find_element(By.CSS_SELECTOR, 'div.css-qoklea')\n",
    "                    link = qoklea_div.find_element(By.CSS_SELECTOR, 'a[href]')\n",
    "                    href = link.get_attribute('href')\n",
    "                    if debug and idx < 3:\n",
    "                        print(f\"  [DEBUG]   Found link in div.css-qoklea, href='{href}'\")\n",
    "                except Exception as e:\n",
    "                    if debug and idx < 3:\n",
    "                        print(f\"  [DEBUG]   div.css-qoklea > a not found: {e}\")\n",
    "                    # Second try: a tag with class chakra-linkbox__overlay\n",
    "                    try:\n",
    "                        link = container.find_element(By.CSS_SELECTOR, 'a.chakra-linkbox__overlay')\n",
    "                        href = link.get_attribute('href')\n",
    "                        if debug and idx < 3:\n",
    "                            print(f\"  [DEBUG]   Found link in chakra-linkbox__overlay, href='{href}'\")\n",
    "                    except:\n",
    "                        # Third try: any a tag with href in the container\n",
    "                        try:\n",
    "                            link = container.find_element(By.CSS_SELECTOR, 'a[href]')\n",
    "                            href = link.get_attribute('href')\n",
    "                            if debug and idx < 3:\n",
    "                                print(f\"  [DEBUG]   Found any a[href] in container, href='{href}'\")\n",
    "                        except:\n",
    "                            pass\n",
    "                \n",
    "                if href:\n",
    "                    # Clean and normalize the URL\n",
    "                    href = href.strip()\n",
    "                    # If it's a relative URL, make it absolute\n",
    "                    if not href.startswith('http'):\n",
    "                        href = urljoin(ROOT_URL, href)\n",
    "                    # Remove any fragments or ensure we have the full URL\n",
    "                    if '#' in href:\n",
    "                        href = href.split('#')[0]\n",
    "                    product_data[\"product-url\"] = href\n",
    "                    if debug and idx < 3:\n",
    "                        print(f\"  [DEBUG] ✓ URL found: '{href}'\")\n",
    "                else:\n",
    "                    if debug and idx < 3:\n",
    "                        print(f\"  [DEBUG] ✗ URL NOT found\")\n",
    "                \n",
    "                # Note: extract_product_details is excluded from this pipeline\n",
    "                \n",
    "                # Only add if we have at least title\n",
    "                if product_data.get(\"product-title\"):\n",
    "                    products.append(product_data)\n",
    "                    if debug and idx < 3:\n",
    "                        print(f\"  [DEBUG] ✓ Product added to list\")\n",
    "            except Exception as e:\n",
    "                if debug and idx < 3:\n",
    "                    print(f\"  [DEBUG] ✗ Error processing container: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"[DEBUG] Page {page_num}: Extracted {len([p for p in products if p.get('product-title')])} products so far\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\n[DEBUG] Total products extracted: {len(products)}\")\n",
    "    \n",
    "    return products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953b6bd",
   "metadata": {},
   "source": [
    "### Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cf0221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Testing extract_products_from_grid() - ALL PAGES\n",
      "======================================================================\n",
      "\n",
      "📍 Testing URL: https://www.loblaws.ca/en/food/dairy-eggs/milk-cream/c/28224?navid=flyout-L3-Milk-and-Cream\n",
      "📍 Category: Milk & Cream\n",
      "\n",
      "======================================================================\n",
      "Starting extraction from ALL pages...\n",
      "======================================================================\n",
      "\n",
      "\n",
      "[DEBUG] Navigating to: https://www.loblaws.ca/en/food/dairy-eggs/milk-cream/c/28224?navid=flyout-L3-Milk-and-Cream\n",
      "[DEBUG] ✓ Listing container found: [data-testid='listing-page-container']\n",
      "[DEBUG] ✓ Grid found: [data-testid='product-grid-component']\n",
      "[DEBUG] ✓ At least one product title found\n",
      "[DEBUG] Scrolled 0 times\n",
      "[DEBUG] ✓ Pagination container found\n",
      "[DEBUG] Found 5 page links\n",
      "[DEBUG]   Link: aria-label='Page 1', href='https://www.loblaws.ca/en/food/dairy-eggs/milk-cream/c/28224?page=1'\n",
      "[DEBUG]   Link: aria-label='Page 2', href='https://www.loblaws.ca/en/food/dairy-eggs/milk-cream/c/28224?page=2'\n",
      "[DEBUG]   Link: aria-label='Page 3', href='https://www.loblaws.ca/en/food/dairy-eggs/milk-cream/c/28224?page=3'\n",
      "[DEBUG]   Link: aria-label='Page 4', href='https://www.loblaws.ca/en/food/dairy-eggs/milk-cream/c/28224?page=4'\n",
      "[DEBUG]   Link: aria-label='Page 5', href='https://www.loblaws.ca/en/food/dairy-eggs/milk-cream/c/28224?page=5'\n",
      "[DEBUG] ✓ Found pagination - Maximum page: 5 (from visible pages: [1, 2, 3, 4, 5])\n",
      "[DEBUG] Will process pages: [1, 2, 3, 4, 5]\n",
      "\n",
      "[DEBUG] Extracting products from page 1/5\n",
      "[DEBUG] Found 96 containers with selector 'div.css-0' within grid\n",
      "[DEBUG] Found 48 product containers (div.css-yyn1h) within div.css-0\n",
      "\n",
      "[DEBUG] Processing container 1/48 on page 1\n",
      "  [DEBUG] ✓ Title found: 'PūrFiltre Milk 2% Partly Skimmed...'\n",
      "  [DEBUG] ✓ Price tile found\n",
      "  [DEBUG] ✓ Regular price found: '$6.00'\n",
      "  [DEBUG] ✓ Size found: '2 l, $0.30/100ml'\n",
      "  [DEBUG]   Found link in div.css-qoklea, href='https://www.loblaws.ca/en/p-rfiltre-milk-2-partly-skimmed/p/21424991_EA?source=nspt'\n",
      "  [DEBUG] ✓ URL found: 'https://www.loblaws.ca/en/p-rfiltre-milk-2-partly-skimmed/p/21424991_EA?source=nspt'\n",
      "  [DEBUG] ✓ Product added to list\n",
      "\n",
      "[DEBUG] Processing container 2/48 on page 1\n",
      "  [DEBUG] ✓ Title found: 'Coffee Creamer, French Vanilla...'\n",
      "  [DEBUG] ✓ Price tile found\n",
      "  [DEBUG] ✓ Regular price found: '$7.00'\n",
      "  [DEBUG] ✓ Size found: '946 ml, $0.74/100ml'\n",
      "  [DEBUG]   Found link in div.css-qoklea, href='https://www.loblaws.ca/en/coffee-creamer-french-vanilla/p/20895480002_EA?source=nspt'\n",
      "  [DEBUG] ✓ URL found: 'https://www.loblaws.ca/en/coffee-creamer-french-vanilla/p/20895480002_EA?source=nspt'\n",
      "  [DEBUG] ✓ Product added to list\n",
      "\n",
      "[DEBUG] Processing container 3/48 on page 1\n",
      "  [DEBUG] ✓ Title found: 'Almond for Coffee, Vanilla Flavour, Plant Based Da...'\n",
      "  [DEBUG] ✓ Price tile found\n",
      "  [DEBUG] ✓ Regular price found: '$5.29'\n",
      "  [DEBUG] ✓ Size found: '890 ml, $0.59/100ml'\n",
      "  [DEBUG]   Found link in div.css-qoklea, href='https://www.loblaws.ca/en/almond-for-coffee-vanilla-flavour-plant-based-dair/p/21245360_EA?source=nspt'\n",
      "  [DEBUG] ✓ URL found: 'https://www.loblaws.ca/en/almond-for-coffee-vanilla-flavour-plant-based-dair/p/21245360_EA?source=nspt'\n",
      "  [DEBUG] ✓ Product added to list\n",
      "[DEBUG] Page 1: Extracted 47 products so far\n",
      "[DEBUG] Navigating to page 2: https://www.loblaws.ca/en/food/dairy-eggs/milk-cream/c/28224?navid=flyout-L3-Milk-and-Cream&page=2\n",
      "[DEBUG] ✓ Navigated to page 2\n",
      "\n",
      "[DEBUG] Extracting products from page 2/5\n",
      "[DEBUG] Found 96 containers with selector 'div.css-0' within grid\n",
      "[DEBUG] Found 48 product containers (div.css-yyn1h) within div.css-0\n",
      "\n",
      "[DEBUG] Processing container 1/48 on page 2\n",
      "  [DEBUG] ✓ Title found: '3.8% Milk...'\n",
      "  [DEBUG] ✓ Price tile found\n",
      "  [DEBUG] ✓ Regular price found: '$8.99'\n",
      "  [DEBUG] ✓ Size found: '2 l, $0.45/100ml'\n",
      "  [DEBUG]   Found link in div.css-qoklea, href='https://www.loblaws.ca/en/3-8-milk/p/20117112_EA?source=nspt'\n",
      "  [DEBUG] ✓ URL found: 'https://www.loblaws.ca/en/3-8-milk/p/20117112_EA?source=nspt'\n",
      "  [DEBUG] ✓ Product added to list\n",
      "\n",
      "[DEBUG] Processing container 2/48 on page 2\n",
      "  [DEBUG] ✓ Title found: 'Oat Creamer, Original...'\n",
      "  [DEBUG] ✓ Price tile found\n",
      "  [DEBUG] ✓ Regular price found: '$6.99'\n",
      "  [DEBUG] ✓ Size found: '946 ml, $0.74/100ml'\n",
      "  [DEBUG]   Found link in div.css-qoklea, href='https://www.loblaws.ca/en/oat-creamer-original/p/21576754_EA?source=nspt'\n",
      "  [DEBUG] ✓ URL found: 'https://www.loblaws.ca/en/oat-creamer-original/p/21576754_EA?source=nspt'\n",
      "  [DEBUG] ✓ Product added to list\n",
      "\n",
      "[DEBUG] Processing container 3/48 on page 2\n",
      "  [DEBUG] ✓ Title found: 'Goat Milk, 2% M.F...'\n",
      "  [DEBUG] ✓ Price tile found\n",
      "  [DEBUG] ✓ Regular price found: '$15.99'\n",
      "  [DEBUG] ✓ Size found: '4 l, $0.40/100ml'\n",
      "  [DEBUG]   Found link in div.css-qoklea, href='https://www.loblaws.ca/en/goat-milk-2-m-f/p/20790474001_EA?source=nspt'\n",
      "  [DEBUG] ✓ URL found: 'https://www.loblaws.ca/en/goat-milk-2-m-f/p/20790474001_EA?source=nspt'\n",
      "  [DEBUG] ✓ Product added to list\n",
      "[DEBUG] Page 2: Extracted 79 products so far\n",
      "[DEBUG] Navigating to page 3: https://www.loblaws.ca/en/food/dairy-eggs/milk-cream/c/28224?navid=flyout-L3-Milk-and-Cream&page=3\n",
      "[DEBUG] ✓ Navigated to page 3\n",
      "\n",
      "[DEBUG] Extracting products from page 3/5\n",
      "[DEBUG] Found 96 containers with selector 'div.css-0' within grid\n",
      "[DEBUG] Found 48 product containers (div.css-yyn1h) within div.css-0\n",
      "\n",
      "[DEBUG] Processing container 1/48 on page 3\n",
      "  [DEBUG] ✓ Title found: '1% Strawberry Partly Skimmed Milk...'\n",
      "  [DEBUG] ⚠ Skipping duplicate or empty title\n",
      "\n",
      "[DEBUG] Processing container 2/48 on page 3\n",
      "  [DEBUG] ✓ Title found: '1% Vanilla Partly Skimmed Milk...'\n",
      "  [DEBUG] ✓ Price tile found\n",
      "  [DEBUG] ✓ Regular price found: '$2.49'\n",
      "  [DEBUG] ✓ Size found: '473 ml, $0.53/100ml'\n",
      "  [DEBUG]   Found link in div.css-qoklea, href='https://www.loblaws.ca/en/1-vanilla-partly-skimmed-milk/p/20895935005_EA?source=nspt'\n",
      "  [DEBUG] ✓ URL found: 'https://www.loblaws.ca/en/1-vanilla-partly-skimmed-milk/p/20895935005_EA?source=nspt'\n",
      "  [DEBUG] ✓ Product added to list\n",
      "\n",
      "[DEBUG] Processing container 3/48 on page 3\n",
      "  [DEBUG] ✓ Title found: '10% Cream...'\n",
      "  [DEBUG] ✓ Price tile found\n",
      "  [DEBUG] ✓ Regular price found: '$5.08'\n",
      "  [DEBUG] ✓ Size found: '1 l, $0.51/100ml'\n",
      "  [DEBUG]   Found link in div.css-qoklea, href='https://www.loblaws.ca/en/10-cream/p/20067088_EA?source=nspt'\n",
      "  [DEBUG] ✓ URL found: 'https://www.loblaws.ca/en/10-cream/p/20067088_EA?source=nspt'\n",
      "  [DEBUG] ✓ Product added to list\n",
      "[DEBUG] Page 3: Extracted 103 products so far\n",
      "[DEBUG] Navigating to page 4: https://www.loblaws.ca/en/food/dairy-eggs/milk-cream/c/28224?navid=flyout-L3-Milk-and-Cream&page=4\n",
      "[DEBUG] ✓ Navigated to page 4\n",
      "\n",
      "[DEBUG] Extracting products from page 4/5\n",
      "[DEBUG] Found 96 containers with selector 'div.css-0' within grid\n",
      "[DEBUG] Found 48 product containers (div.css-yyn1h) within div.css-0\n",
      "\n",
      "[DEBUG] Processing container 1/48 on page 4\n",
      "  [DEBUG] ✓ Title found: 'Café Mocha Coffee & Hot Chocolate Mix Instant Coff...'\n",
      "  [DEBUG] ✓ Price tile found\n",
      "  [DEBUG] ✓ Regular price found: '$7.99'\n",
      "  [DEBUG] ✓ Size found: '8 ea, $1.00/1ea'\n",
      "  [DEBUG]   Found link in div.css-qoklea, href='https://www.loblaws.ca/en/caf-mocha-coffee-hot-chocolate-mix-instant-coffee/p/21431715_EA?source=nspt'\n",
      "  [DEBUG] ✓ URL found: 'https://www.loblaws.ca/en/caf-mocha-coffee-hot-chocolate-mix-instant-coffee/p/21431715_EA?source=nspt'\n",
      "  [DEBUG] ✓ Product added to list\n",
      "\n",
      "[DEBUG] Processing container 2/48 on page 4\n",
      "  [DEBUG] ✓ Title found: 'Caramilk Chocolate Milkshake...'\n",
      "  [DEBUG] ✓ Price tile found\n",
      "  [DEBUG] ✓ Regular price found: '$2.99'\n",
      "  [DEBUG] ✓ Size found: '310 ml, $0.96/100ml'\n",
      "  [DEBUG]   Found link in div.css-qoklea, href='https://www.loblaws.ca/en/caramilk-chocolate-milkshake/p/21040942_EA?source=nspt'\n",
      "  [DEBUG] ✓ URL found: 'https://www.loblaws.ca/en/caramilk-chocolate-milkshake/p/21040942_EA?source=nspt'\n",
      "  [DEBUG] ✓ Product added to list\n",
      "\n",
      "[DEBUG] Processing container 3/48 on page 4\n",
      "  [DEBUG] ✓ Title found: 'Chocolate High Protein Shake...'\n",
      "  [DEBUG] ✓ Price tile found\n",
      "  [DEBUG] ✓ Regular price found: '$3.69'\n",
      "  [DEBUG] ✓ Size found: '460 ml, $0.80/100ml'\n",
      "  [DEBUG]   Found link in div.css-qoklea, href='https://www.loblaws.ca/en/chocolate-high-protein-shake/p/21345511_EA?source=nspt'\n",
      "  [DEBUG] ✓ URL found: 'https://www.loblaws.ca/en/chocolate-high-protein-shake/p/21345511_EA?source=nspt'\n",
      "  [DEBUG] ✓ Product added to list\n",
      "[DEBUG] Page 4: Extracted 139 products so far\n",
      "[DEBUG] Navigating to page 5: https://www.loblaws.ca/en/food/dairy-eggs/milk-cream/c/28224?navid=flyout-L3-Milk-and-Cream&page=5\n",
      "[DEBUG] ✓ Navigated to page 5\n",
      "\n",
      "[DEBUG] Extracting products from page 5/5\n",
      "[DEBUG] Found 88 containers with selector 'div.css-0' within grid\n",
      "[DEBUG] Found 44 product containers (div.css-yyn1h) within div.css-0\n",
      "\n",
      "[DEBUG] Processing container 1/44 on page 5\n",
      "  [DEBUG] ✓ Title found: 'Lactose Free 2% Milk...'\n",
      "  [DEBUG] ✓ Price tile found\n",
      "  [DEBUG] ✓ Regular price found: '$6.00'\n",
      "  [DEBUG] ✓ Size found: '2 l, $0.30/100ml'\n",
      "  [DEBUG]   Found link in div.css-qoklea, href='https://www.loblaws.ca/en/lactose-free-2-milk/p/21441429_EA?source=nspt'\n",
      "  [DEBUG] ✓ URL found: 'https://www.loblaws.ca/en/lactose-free-2-milk/p/21441429_EA?source=nspt'\n",
      "  [DEBUG] ✓ Product added to list\n",
      "\n",
      "[DEBUG] Processing container 2/44 on page 5\n",
      "  [DEBUG] ✓ Title found: 'Lactose Free Chocolate Milk...'\n",
      "  [DEBUG] ✓ Price tile found\n",
      "  [DEBUG] ✓ Regular price found: '$7.48'\n",
      "  [DEBUG] ✓ Size found: '2 l, $0.37/100ml'\n",
      "  [DEBUG]   Found link in div.css-qoklea, href='https://www.loblaws.ca/en/lactose-free-chocolate-milk/p/21040983_EA?source=nspt'\n",
      "  [DEBUG] ✓ URL found: 'https://www.loblaws.ca/en/lactose-free-chocolate-milk/p/21040983_EA?source=nspt'\n",
      "  [DEBUG] ✓ Product added to list\n",
      "\n",
      "[DEBUG] Processing container 3/44 on page 5\n",
      "  [DEBUG] ✓ Title found: 'Lactose Free Fat Free Skim 0% Dairy Product (2L)...'\n",
      "  [DEBUG] ✓ Price tile found\n",
      "  [DEBUG] ✓ Regular price found: '$7.14'\n",
      "  [DEBUG] ✓ Size found: '2 l, $0.36/100ml'\n",
      "  [DEBUG]   Found link in div.css-qoklea, href='https://www.loblaws.ca/en/lactose-free-fat-free-skim-0-dairy-product-2l/p/20077874004_EA?source=nspt'\n",
      "  [DEBUG] ✓ URL found: 'https://www.loblaws.ca/en/lactose-free-fat-free-skim-0-dairy-product-2l/p/20077874004_EA?source=nspt'\n",
      "  [DEBUG] ✓ Product added to list\n",
      "[DEBUG] Page 5: Extracted 178 products so far\n",
      "\n",
      "[DEBUG] Total products extracted: 178\n",
      "\n",
      "======================================================================\n",
      "EXTRACTION SUMMARY\n",
      "======================================================================\n",
      "✓ Total products extracted: 178\n",
      "\n",
      "📦 Sample products (first 10):\n",
      "  1. PūrFiltre Milk 2% Partly Skimmed\n",
      "      Price: $6.00\n",
      "      Size: 2 l, $0.30/100ml\n",
      "  2. Coffee Creamer, French Vanilla\n",
      "      Price: $7.00\n",
      "      Size: 946 ml, $0.74/100ml\n",
      "  3. Almond for Coffee, Vanilla Flavour, Plant Based Dairy Free Coffee Creamer 890ml\n",
      "      Price: $5.29\n",
      "      Size: 890 ml, $0.59/100ml\n",
      "  4. Coffee Creamer, Hazelnut\n",
      "      Price: $7.00\n",
      "      Size: 946 ml, $0.74/100ml\n",
      "  5. Oat for Coffee, Vanilla, Plant Based, Dairy Free Coffee Creamer\n",
      "      Price: $5.29\n",
      "      Size: 890 ml, $0.59/100ml\n",
      "  6. Oat Coffee Creamer, Peppermint Mocha, Limited Edition\n",
      "      Price: $2.79\n",
      "      Size: 450 ml, $0.62/100ml\n",
      "  7. Vanilla Toffee Caramel Coffee Creamer, 63 Servings\n",
      "      Price: $7.00\n",
      "      Size: 946 ml, $0.74/100ml\n",
      "  8. Chocolate Milk\n",
      "      Price: $3.70\n",
      "      Size: 1 l, $0.37/100ml\n",
      "  9. Coffee Creamer, Southern Butter Pecan\n",
      "      Price: $7.00\n",
      "      Size: 946 ml, $0.74/100ml\n",
      "  10. French Vanilla Flavoured Fat Free Coffee Creamer, 63 Servings\n",
      "      Price: $7.00\n",
      "      Size: 946 ml, $0.74/100ml\n",
      "\n",
      "  ... and 168 more products\n",
      "\n",
      "📊 Statistics:\n",
      "  - Products with title: 178\n",
      "  - Products with price: 178\n",
      "  - Products with size: 178\n",
      "  - Products with URL: 178\n",
      "  - Complete products (title + URL): 178\n",
      "\n",
      "✓ Test PASSED! Successfully extracted 178 products from all pages.\n"
     ]
    }
   ],
   "source": [
    "### Test extract_products_from_grid - All Pages\n",
    "\n",
    "# Test extract_products_from_grid with pagination\n",
    "driver = get_driver(headless=False)\n",
    "wait = WebDriverWait(driver, MAX_WAIT)\n",
    "\n",
    "try:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Testing extract_products_from_grid() - ALL PAGES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get a subcategory2 URL from the dropdown\n",
    "    categories = extract_main_categories(driver, wait)\n",
    "    category_name = \"GROCERY\"\n",
    "    subcategory_name = \"Dairy & Eggs\"\n",
    "    \n",
    "    category_data = next((c for c in categories if category_name.upper() in c['name'].upper()), None)\n",
    "    if not category_data or not category_data.get('aria-controls'):\n",
    "        print(f\"\\n✗ Could not find {category_name} or aria-controls\")\n",
    "    else:\n",
    "        aria_controls = category_data['aria-controls']\n",
    "        \n",
    "        # Get subcategory2 items\n",
    "        subcategory2_list = extract_subcategory2_from_dropdown(driver, wait, category_name, subcategory_name, aria_controls)\n",
    "        \n",
    "        if not subcategory2_list:\n",
    "            print(f\"\\n✗ No subcategory2 items found for {subcategory_name}\")\n",
    "        else:\n",
    "            # Use the first subcategory2 URL\n",
    "            test_url = subcategory2_list[0]['url']\n",
    "            print(f\"\\n📍 Testing URL: {test_url}\")\n",
    "            print(f\"📍 Category: {subcategory2_list[0]['name']}\")\n",
    "            print(f\"\\n{'=' * 70}\")\n",
    "            print(\"Starting extraction from ALL pages...\")\n",
    "            print(f\"{'=' * 70}\\n\")\n",
    "            \n",
    "            # Test the function with debug enabled\n",
    "            products = extract_products_from_grid(driver, wait, test_url, debug=True)\n",
    "            \n",
    "            print(f\"\\n{'=' * 70}\")\n",
    "            print(f\"EXTRACTION SUMMARY\")\n",
    "            print(f\"{'=' * 70}\")\n",
    "            print(f\"✓ Total products extracted: {len(products)}\")\n",
    "            \n",
    "            # Show sample products\n",
    "            if len(products) > 0:\n",
    "                print(f\"\\n📦 Sample products (first 10):\")\n",
    "                for i, product in enumerate(products[:10], 1):\n",
    "                    print(f\"  {i}. {product.get('product-title', 'N/A')}\")\n",
    "                    print(f\"      Price: {product.get('regular-price', 'N/A')}\")\n",
    "                    print(f\"      Size: {product.get('product-package-size', 'N/A')}\")\n",
    "                \n",
    "                if len(products) > 10:\n",
    "                    print(f\"\\n  ... and {len(products) - 10} more products\")\n",
    "                \n",
    "                # Count products with all fields\n",
    "                complete_products = [p for p in products if p.get('product-title') and p.get('product-url')]\n",
    "                print(f\"\\n📊 Statistics:\")\n",
    "                print(f\"  - Products with title: {len([p for p in products if p.get('product-title')])}\")\n",
    "                print(f\"  - Products with price: {len([p for p in products if p.get('regular-price')])}\")\n",
    "                print(f\"  - Products with size: {len([p for p in products if p.get('product-package-size')])}\")\n",
    "                print(f\"  - Products with URL: {len([p for p in products if p.get('product-url')])}\")\n",
    "                print(f\"  - Complete products (title + URL): {len(complete_products)}\")\n",
    "                \n",
    "                print(f\"\\n✓ Test PASSED! Successfully extracted {len(products)} products from all pages.\")\n",
    "            else:\n",
    "                print(f\"\\n⚠ Test completed but no products found.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Test FAILED: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaceaed0",
   "metadata": {},
   "source": [
    "### Extracting product details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_details(driver, wait, product_url):\n",
    "    \"\"\"Extract product details from product page.\"\"\"\n",
    "    driver.get(product_url)\n",
    "    time.sleep(2)\n",
    "    dismiss_overlays(driver)\n",
    "    \n",
    "    details = {\"nutrition-info\": \"\", \"description\": \"\", \"legal-disclaimer\": \"\"}\n",
    "    \n",
    "    try:\n",
    "        container = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid=\"product-details-page-details\"], [data-testid=\"product-details-accordion\"]')))\n",
    "    except:\n",
    "        container = driver.find_element(By.CSS_SELECTOR, 'body')\n",
    "    \n",
    "    # Nutrition\n",
    "    try:\n",
    "        nutrition = container.find_element(By.CSS_SELECTOR, 'div.product-details-page-nutrition-info, [class*=\"nutrition\"]')\n",
    "        details[\"nutrition-info\"] = nutrition.text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Description\n",
    "    try:\n",
    "        desc = container.find_element(By.CSS_SELECTOR, 'div.product-details-page-description, [class*=\"description\"]')\n",
    "        details[\"description\"] = desc.text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Disclaimer\n",
    "    try:\n",
    "        disclaimer = container.find_element(By.CSS_SELECTOR, 'div.product-details-page-legal-disclaimer, [class*=\"disclaimer\"]')\n",
    "        details[\"legal-disclaimer\"] = disclaimer.text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05980950",
   "metadata": {},
   "source": [
    "### Test the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90239ff",
   "metadata": {},
   "source": [
    "### Pipeline: Extract all products from GROCERY and HOME, BEAUTY & BABY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c766806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Improved JSON Export Function\n",
    "\n",
    "def export_products_to_json(products, output_file, extraction_stats, target_categories):\n",
    "    \"\"\"\n",
    "    Export products to a clean, well-structured JSON format matching the test output.\n",
    "    \n",
    "    Args:\n",
    "        products: List of product dictionaries\n",
    "        output_file: Path to output JSON file\n",
    "        extraction_stats: Dictionary with extraction statistics\n",
    "        target_categories: List of target categories processed\n",
    "    \n",
    "    Returns:\n",
    "        dict: The output data structure\n",
    "    \"\"\"\n",
    "    # Clean and structure the products data - match the test output format\n",
    "    cleaned_products = []\n",
    "    for product in products:\n",
    "        cleaned_product = {\n",
    "            \"product-title\": product.get(\"product-title\", \"\"),\n",
    "            \"regular-price\": product.get(\"regular-price\", \"\"),\n",
    "            \"product-package-size\": product.get(\"product-package-size\", \"\"),\n",
    "            \"product-url\": product.get(\"product-url\", \"\"),\n",
    "            \"category\": product.get(\"category\", \"\"),\n",
    "            \"subcategory\": product.get(\"subcategory\", \"\"),\n",
    "            \"subcategory2\": product.get(\"subcategory2\", \"\")\n",
    "        }\n",
    "        # Only add products that have at least a title\n",
    "        if cleaned_product[\"product-title\"]:\n",
    "            cleaned_products.append(cleaned_product)\n",
    "    \n",
    "    # Create well-structured JSON output matching test format\n",
    "    output_data = {\n",
    "        \"extraction_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"extraction_summary\": {\n",
    "            \"categories_processed\": target_categories,\n",
    "            \"total_categories\": extraction_stats.get(\"categories_processed\", 0),\n",
    "            \"total_subcategories\": extraction_stats.get(\"subcategories_processed\", 0),\n",
    "            \"total_subcategory2\": extraction_stats.get(\"subcategory2_processed\", 0),\n",
    "            \"total_products\": len(cleaned_products),\n",
    "            \"errors_count\": len(extraction_stats.get(\"errors\", []))\n",
    "        },\n",
    "        \"statistics\": {\n",
    "            \"products_with_title\": len([p for p in cleaned_products if p.get(\"product-title\")]),\n",
    "            \"products_with_price\": len([p for p in cleaned_products if p.get(\"regular-price\")]),\n",
    "            \"products_with_size\": len([p for p in cleaned_products if p.get(\"product-package-size\")]),\n",
    "            \"products_with_url\": len([p for p in cleaned_products if p.get(\"product-url\")]),\n",
    "            \"complete_products\": len([p for p in cleaned_products if p.get(\"product-title\") and p.get(\"product-url\")])\n",
    "        },\n",
    "        \"products\": cleaned_products\n",
    "    }\n",
    "    \n",
    "    # Add errors if any (limit to first 50 to keep file size manageable)\n",
    "    if extraction_stats.get(\"errors\"):\n",
    "        output_data[\"errors\"] = extraction_stats[\"errors\"][:50]\n",
    "    \n",
    "    # Write to JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✓ Data exported to: {output_file}\")\n",
    "    print(f\"✓ Total products: {len(cleaned_products)}\")\n",
    "    print(f\"✓ Products with title: {output_data['statistics']['products_with_title']}\")\n",
    "    print(f\"✓ Products with price: {output_data['statistics']['products_with_price']}\")\n",
    "    print(f\"✓ Products with size: {output_data['statistics']['products_with_size']}\")\n",
    "    print(f\"✓ Products with URL: {output_data['statistics']['products_with_url']}\")\n",
    "    print(f\"✓ Complete products: {output_data['statistics']['complete_products']}\")\n",
    "    \n",
    "    return output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d0ed6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_products_pipeline(driver, wait, output_file=\"loblaws_products.json\", debug=False):\n",
    "    \"\"\"\n",
    "    Pipeline to extract all products from GROCERY and HOME, BEAUTY & BABY categories.\n",
    "    \n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance\n",
    "        wait: WebDriverWait instance\n",
    "        output_file: Path to output JSON file\n",
    "        debug: Enable debug output\n",
    "    \n",
    "    Returns:\n",
    "        dict: Summary of extraction with total products and statistics\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"STARTING PRODUCT EXTRACTION PIPELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Target categories: {TARGET_CATEGORIES}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    all_products = []\n",
    "    extraction_stats = {\n",
    "        \"categories_processed\": 0,\n",
    "        \"subcategories_processed\": 0,\n",
    "        \"subcategory2_processed\": 0,\n",
    "        \"total_products\": 0,\n",
    "        \"errors\": []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Get main categories\n",
    "        if debug:\n",
    "            print(\"\\n[STEP 1] Extracting main categories...\")\n",
    "        categories = extract_main_categories(driver, wait)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"✓ Found {len(categories)} main categories\")\n",
    "        \n",
    "        # Step 2: Filter for target categories\n",
    "        target_category_data = []\n",
    "        for cat in categories:\n",
    "            for target in TARGET_CATEGORIES:\n",
    "                if target.upper() in cat['name'].upper():\n",
    "                    target_category_data.append(cat)\n",
    "                    break\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"✓ Found {len(target_category_data)} target categories:\")\n",
    "            for cat in target_category_data:\n",
    "                print(f\"  - {cat['name']}\")\n",
    "        \n",
    "        if not target_category_data:\n",
    "            print(\"\\n✗ No target categories found!\")\n",
    "            return extraction_stats\n",
    "        \n",
    "        # Step 3: Process each target category\n",
    "        for category_idx, category_data in enumerate(target_category_data, 1):\n",
    "            category_name = category_data['name']\n",
    "            aria_controls = category_data.get('aria-controls')\n",
    "            \n",
    "            if not aria_controls:\n",
    "                error_msg = f\"Category '{category_name}' has no aria-controls\"\n",
    "                extraction_stats[\"errors\"].append(error_msg)\n",
    "                if debug:\n",
    "                    print(f\"\\n⚠ {error_msg}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\n{'=' * 70}\")\n",
    "            print(f\"PROCESSING CATEGORY {category_idx}/{len(target_category_data)}: {category_name}\")\n",
    "            print(f\"{'=' * 70}\")\n",
    "            \n",
    "            extraction_stats[\"categories_processed\"] += 1\n",
    "            \n",
    "            # Step 4: Get subcategories for this category\n",
    "            if debug:\n",
    "                print(f\"\\n[STEP 4] Extracting subcategories for '{category_name}'...\")\n",
    "            \n",
    "            try:\n",
    "                subcategories = extract_subcategories(driver, wait, category_name, aria_controls)\n",
    "                \n",
    "                if debug:\n",
    "                    print(f\"✓ Found {len(subcategories)} subcategories\")\n",
    "                \n",
    "                if not subcategories:\n",
    "                    if debug:\n",
    "                        print(f\"⚠ No subcategories found for '{category_name}'\")\n",
    "                    continue\n",
    "                \n",
    "                # Step 5: OPTIMIZED - Extract all subcategory2 items for all subcategories in one batch\n",
    "                # This avoids repeatedly opening/closing the dropdown\n",
    "                if debug:\n",
    "                    print(f\"\\n[STEP 5] Batch extracting subcategory2 items for all {len(subcategories)} subcategories...\")\n",
    "                \n",
    "                try:\n",
    "                    subcategory2_map = extract_all_subcategory2_for_category(\n",
    "                        driver, wait, category_name, subcategories, aria_controls\n",
    "                    )\n",
    "                    \n",
    "                    if debug:\n",
    "                        total_subcat2 = sum(len(items) for items in subcategory2_map.values())\n",
    "                        print(f\"✓ Batch extraction complete: Found {total_subcat2} subcategory2 items across {len(subcategory2_map)} subcategories\")\n",
    "                    \n",
    "                    # Step 6: Process all subcategory2 items and extract products\n",
    "                    # Now we can process all products sequentially without going back to dropdown\n",
    "                    total_subcat2_count = sum(len(items) for items in subcategory2_map.values())\n",
    "                    current_subcat2_count = 0\n",
    "                    \n",
    "                    for subcat_idx, subcategory in enumerate(subcategories, 1):\n",
    "                        subcategory_name = subcategory['name']\n",
    "                        subcategory2_list = subcategory2_map.get(subcategory_name, [])\n",
    "                        \n",
    "                        if not subcategory2_list:\n",
    "                            if debug:\n",
    "                                print(f\"\\n  ⚠ No subcategory2 items found for '{subcategory_name}'\")\n",
    "                            extraction_stats[\"subcategories_processed\"] += 1\n",
    "                            continue\n",
    "                        \n",
    "                        print(f\"\\n  Processing subcategory {subcat_idx}/{len(subcategories)}: {subcategory_name} ({len(subcategory2_list)} subcategory2 items)\")\n",
    "                        extraction_stats[\"subcategories_processed\"] += 1\n",
    "                        \n",
    "                        # Process each subcategory2 and extract products\n",
    "                        for subcat2_idx, subcat2_item in enumerate(subcategory2_list, 1):\n",
    "                            current_subcat2_count += 1\n",
    "                            subcat2_name = subcat2_item['name']\n",
    "                            subcat2_url = subcat2_item['url']\n",
    "                            \n",
    "                            print(f\"    [{current_subcat2_count}/{total_subcat2_count}] Extracting products from: {subcat2_name}\")\n",
    "                            \n",
    "                            extraction_stats[\"subcategory2_processed\"] += 1\n",
    "                            \n",
    "                            try:\n",
    "                                # Extract products from all pages\n",
    "                                products = extract_products_from_grid(driver, wait, subcat2_url, debug=False)\n",
    "                                \n",
    "                                # Add metadata to each product\n",
    "                                for product in products:\n",
    "                                    product[\"category\"] = category_name\n",
    "                                    product[\"subcategory\"] = subcategory_name\n",
    "                                    product[\"subcategory2\"] = subcat2_name\n",
    "                                \n",
    "                                all_products.extend(products)\n",
    "                                \n",
    "                                print(f\"      ✓ Extracted {len(products)} products\")\n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                error_msg = f\"Error extracting products from '{subcat2_name}': {str(e)}\"\n",
    "                                extraction_stats[\"errors\"].append(error_msg)\n",
    "                                if debug:\n",
    "                                    print(f\"      ✗ {error_msg}\")\n",
    "                                    import traceback\n",
    "                                    traceback.print_exc()\n",
    "                                else:\n",
    "                                    print(f\"      ✗ Error: {str(e)[:100]}\")\n",
    "                                continue\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error in batch extracting subcategory2 for '{category_name}': {str(e)}\"\n",
    "                    extraction_stats[\"errors\"].append(error_msg)\n",
    "                    if debug:\n",
    "                        print(f\"  ✗ {error_msg}\")\n",
    "                        import traceback\n",
    "                        traceback.print_exc()\n",
    "                    # Fallback: try individual extraction for each subcategory\n",
    "                    print(f\"  ⚠ Falling back to individual subcategory2 extraction...\")\n",
    "                    for subcat_idx, subcategory in enumerate(subcategories, 1):\n",
    "                        subcategory_name = subcategory['name']\n",
    "                        extraction_stats[\"subcategories_processed\"] += 1\n",
    "                        \n",
    "                        try:\n",
    "                            subcategory2_list = extract_subcategory2_from_dropdown(\n",
    "                                driver, wait, category_name, subcategory_name, aria_controls\n",
    "                            )\n",
    "                            \n",
    "                            if not subcategory2_list:\n",
    "                                continue\n",
    "                            \n",
    "                            for subcat2_item in subcategory2_list:\n",
    "                                subcat2_name = subcat2_item['name']\n",
    "                                subcat2_url = subcat2_item['url']\n",
    "                                extraction_stats[\"subcategory2_processed\"] += 1\n",
    "                                \n",
    "                                try:\n",
    "                                    products = extract_products_from_grid(driver, wait, subcat2_url, debug=False)\n",
    "                                    for product in products:\n",
    "                                        product[\"category\"] = category_name\n",
    "                                        product[\"subcategory\"] = subcategory_name\n",
    "                                        product[\"subcategory2\"] = subcat2_name\n",
    "                                    all_products.extend(products)\n",
    "                                    print(f\"      ✓ Extracted {len(products)} products from {subcat2_name}\")\n",
    "                                except Exception as e:\n",
    "                                    extraction_stats[\"errors\"].append(f\"Error extracting products from '{subcat2_name}': {str(e)}\")\n",
    "                                    continue\n",
    "                        except:\n",
    "                            continue\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error extracting subcategories for '{category_name}': {str(e)}\"\n",
    "                extraction_stats[\"errors\"].append(error_msg)\n",
    "                if debug:\n",
    "                    print(f\"  ✗ {error_msg}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # Step 8: Export to JSON\n",
    "        extraction_stats[\"total_products\"] = len(all_products)\n",
    "        \n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(\"EXPORTING DATA TO JSON\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        \n",
    "        output_data = {\n",
    "            \"extraction_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"categories_processed\": TARGET_CATEGORIES,\n",
    "            \"statistics\": extraction_stats,\n",
    "            \"products\": all_products\n",
    "        }\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"✓ Data exported to: {output_file}\")\n",
    "        print(f\"✓ Total products: {len(all_products)}\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(\"EXTRACTION SUMMARY\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        print(f\"Categories processed: {extraction_stats['categories_processed']}\")\n",
    "        print(f\"Subcategories processed: {extraction_stats['subcategories_processed']}\")\n",
    "        print(f\"Subcategory2 processed: {extraction_stats['subcategory2_processed']}\")\n",
    "        print(f\"Total products extracted: {extraction_stats['total_products']}\")\n",
    "        print(f\"Errors encountered: {len(extraction_stats['errors'])}\")\n",
    "        \n",
    "        if extraction_stats['errors']:\n",
    "            print(f\"\\n⚠ Errors encountered:\")\n",
    "            for i, error in enumerate(extraction_stats['errors'][:10], 1):\n",
    "                print(f\"  {i}. {error}\")\n",
    "            if len(extraction_stats['errors']) > 10:\n",
    "                print(f\"  ... and {len(extraction_stats['errors']) - 10} more errors\")\n",
    "        \n",
    "        print(f\"\\n{'=' * 70}\")\n",
    "        print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        print(f\"{'=' * 70}\")\n",
    "        \n",
    "        return extraction_stats\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Pipeline failed: {str(e)}\"\n",
    "        extraction_stats[\"errors\"].append(error_msg)\n",
    "        print(f\"\\n✗ {error_msg}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return extraction_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb186fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING PRODUCT EXTRACTION PIPELINE\n",
      "======================================================================\n",
      "Target categories: ['GROCERY', 'HOME, BEAUTY & BABY']\n",
      "Output file: loblaws_products.json\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PROCESSING CATEGORY 1/2: GROCERY\n",
      "======================================================================\n",
      "\n",
      "  Processing subcategory 1/15: Fruits & Vegetables (7 subcategory2 items)\n",
      "    [1/157] Extracting products from: Fresh Vegetables\n",
      "      ✓ Extracted 317 products\n",
      "    [2/157] Extracting products from: Fresh Fruits\n",
      "      ✓ Extracted 175 products\n",
      "    [3/157] Extracting products from: Packaged Salads & Dressing\n",
      "      ✓ Extracted 120 products\n",
      "    [4/157] Extracting products from: Herbs\n",
      "      ✓ Extracted 0 products\n",
      "    [5/157] Extracting products from: Fresh Cut Fruits & Vegetables\n",
      "      ✓ Extracted 65 products\n",
      "    [6/157] Extracting products from: Dried Fruits & Nuts\n",
      "      ✓ Extracted 53 products\n",
      "    [7/157] Extracting products from: Fresh Juice & Smoothies\n",
      "      ✓ Extracted 80 products\n",
      "\n",
      "  Processing subcategory 2/15: Dairy & Eggs (9 subcategory2 items)\n",
      "    [8/157] Extracting products from: Milk & Cream\n",
      "      ✓ Extracted 178 products\n",
      "    [9/157] Extracting products from: Egg & Egg Substitutes\n",
      "      ✓ Extracted 42 products\n",
      "    [10/157] Extracting products from: Butter & Spreads\n",
      "      ✓ Extracted 68 products\n",
      "    [11/157] Extracting products from: Cheese\n",
      "      ✓ Extracted 262 products\n",
      "    [12/157] Extracting products from: Yogurt\n",
      "      ✓ Extracted 339 products\n",
      "    [13/157] Extracting products from: Desserts & Doughs\n",
      "      ✓ Extracted 45 products\n",
      "    [14/157] Extracting products from: Sour Cream & Dips\n",
      "      ✓ Extracted 25 products\n",
      "    [15/157] Extracting products from: Lactose Free\n",
      "      ✓ Extracted 40 products\n",
      "    [16/157] Extracting products from: Non-Dairy Milk Alternatives\n",
      "      ✓ Extracted 102 products\n",
      "\n",
      "  Processing subcategory 3/15: Meat (11 subcategory2 items)\n",
      "    [17/157] Extracting products from: Chicken & Turkey\n",
      "      ✓ Extracted 173 products\n",
      "    [18/157] Extracting products from: Beef\n",
      "      ✓ Extracted 129 products\n",
      "    [19/157] Extracting products from: Sausages\n",
      "      ✓ Extracted 65 products\n",
      "    [20/157] Extracting products from: Bacon\n",
      "      ✓ Extracted 38 products\n",
      "    [21/157] Extracting products from: Hot Dogs\n",
      "      ✓ Extracted 31 products\n",
      "    [22/157] Extracting products from: Pork & Ham\n",
      "      ✓ Extracted 70 products\n",
      "    [23/157] Extracting products from: Lamb & Veal\n",
      "      ✓ Extracted 35 products\n",
      "    [24/157] Extracting products from: Kebabs & Marinated Meat\n",
      "      ✓ Extracted 39 products\n",
      "    [25/157] Extracting products from: Game Meat, Offals & Fowl\n",
      "      ✓ Extracted 20 products\n",
      "    [26/157] Extracting products from: Plant-Based Meat Alternatives\n",
      "      ✓ Extracted 28 products\n",
      "    [27/157] Extracting products from: Deli Meat\n",
      "      ✓ Extracted 228 products\n",
      "\n",
      "  Processing subcategory 4/15: Pantry (13 subcategory2 items)\n",
      "    [28/157] Extracting products from: Canned & Pickled\n",
      "      ✓ Extracted 759 products\n",
      "    [29/157] Extracting products from: Baking Essentials\n",
      "      ✓ Extracted 670 products\n",
      "    [30/157] Extracting products from: Pasta & Pasta Sauce\n",
      "      ✓ Extracted 618 products\n",
      "    [31/157] Extracting products from: Easy Meals & Sides\n",
      "      ✓ Extracted 280 products\n",
      "    [32/157] Extracting products from: Cereal & Breakfast\n",
      "      ✓ Extracted 235 products\n",
      "    [33/157] Extracting products from: Honey, Syrups & Spreads\n",
      "      ✓ Extracted 179 products\n",
      "    [34/157] Extracting products from: Rice\n",
      "      ✓ Extracted 158 products\n",
      "    [35/157] Extracting products from: Oils & Vinegar\n",
      "      ✓ Extracted 210 products\n",
      "    [36/157] Extracting products from: Condiments\n",
      "      ✓ Extracted 942 products\n",
      "    [37/157] Extracting products from: Spices & Seasonings\n",
      "      ✓ Extracted 435 products\n",
      "    [38/157] Extracting products from: Dried Beans, Vegetables & Grains\n",
      "      ✓ Extracted 54 products\n",
      "    [39/157] Extracting products from: Bulk Nuts and Candy\n",
      "      ✓ Extracted 261 products\n",
      "    [40/157] Extracting products from: International Foods\n",
      "      ✓ Extracted 261 products\n",
      "\n",
      "  Processing subcategory 5/15: International Foods (47 subcategory2 items)\n",
      "    [41/157] Extracting products from: East Asian Foods\n",
      "      ✓ Extracted 620 products\n",
      "    [42/157] Extracting products from: Rice & Grain\n",
      "      ✓ Extracted 26 products\n",
      "    [43/157] Extracting products from: Noodles & Noodle Soup\n",
      "      ✓ Extracted 104 products\n",
      "    [44/157] Extracting products from: Snacks, Cookies & Crackers\n",
      "      ✓ Extracted 91 products\n",
      "    [45/157] Extracting products from: Condiments, Sauces & Oil\n",
      "      ✓ Extracted 95 products\n",
      "    [46/157] Extracting products from: Shop More East Asian Foods\n",
      "      ✓ Extracted 625 products\n",
      "    [47/157] Extracting products from: South Asian Foods\n",
      "      ✓ Extracted 0 products\n",
      "    [48/157] Extracting products from: Rice, Lentils & Beans\n",
      "      ✓ Extracted 35 products\n",
      "    [49/157] Extracting products from: Snacks, Cookies & Crackers\n",
      "      ✓ Extracted 43 products\n",
      "    [50/157] Extracting products from: Bakery\n",
      "      ✓ Extracted 19 products\n",
      "    [51/157] Extracting products from: Spices & Seasonings\n",
      "      ✓ Extracted 54 products\n",
      "    [52/157] Extracting products from: Shop More South Asian Foods\n",
      "      ✓ Extracted 325 products\n",
      "    [53/157] Extracting products from: Caribbean Foods\n",
      "      ✓ Extracted 112 products\n",
      "    [54/157] Extracting products from: Rice, Lentils & Beans\n",
      "      ✓ Extracted 4 products\n",
      "    [55/157] Extracting products from: Snacks, Cookies & Crackers\n",
      "      ✓ Extracted 4 products\n",
      "    [56/157] Extracting products from: Baking & Flour\n",
      "      ✓ Extracted 4 products\n",
      "    [57/157] Extracting products from: Spices & Seasonings\n",
      "      ✓ Extracted 7 products\n",
      "    [58/157] Extracting products from: Shop More Caribbean Foods\n",
      "      ✓ Extracted 112 products\n",
      "    [59/157] Extracting products from: Halal Foods\n",
      "      ✓ Extracted 73 products\n",
      "    [60/157] Extracting products from: Meat\n",
      "      ✓ Extracted 59 products\n",
      "    [61/157] Extracting products from: Deli\n",
      "      ✓ Extracted 8 products\n",
      "    [62/157] Extracting products from: Dairy\n",
      "      ✓ Extracted 11 products\n",
      "    [63/157] Extracting products from: Frozen Entrees\n",
      "      ✓ Extracted 0 products\n",
      "    [64/157] Extracting products from: Middle Eastern Foods\n",
      "      ✓ Extracted 145 products\n",
      "    [65/157] Extracting products from: Canned, Pickled Food & Olives\n",
      "      ✓ Extracted 24 products\n",
      "    [66/157] Extracting products from: Snacks, Cookies & Crackers\n",
      "      ✓ Extracted 30 products\n",
      "    [67/157] Extracting products from: Frozen\n",
      "      ✓ Extracted 0 products\n",
      "    [68/157] Extracting products from: Shop More Middle Eastern Foods\n",
      "      ✓ Extracted 145 products\n",
      "    [69/157] Extracting products from: Filipino Foods\n",
      "      ✓ Extracted 71 products\n",
      "    [70/157] Extracting products from: Rice, Noodles & Noodle Soup\n",
      "      ✓ Extracted 15 products\n",
      "    [71/157] Extracting products from: Snacks, Cookies & Crackers\n",
      "      ✓ Extracted 2 products\n",
      "    [72/157] Extracting products from: Canned Foods\n",
      "      ✓ Extracted 19 products\n",
      "    [73/157] Extracting products from: Shop More Filipino Foods\n",
      "      ✓ Extracted 71 products\n",
      "    [74/157] Extracting products from: Latin American Foods\n",
      "      ✓ Extracted 52 products\n",
      "    [75/157] Extracting products from: Condiments & Salsa\n",
      "      ✓ Extracted 30 products\n",
      "    [76/157] Extracting products from: Hot & Cold Drinks\n",
      "      ✓ Extracted 6 products\n",
      "    [77/157] Extracting products from: Fruits & Vegetables\n",
      "      ✓ Extracted 6 products\n",
      "    [78/157] Extracting products from: Shop More Latin American Foods\n",
      "      ✓ Extracted 52 products\n",
      "    [79/157] Extracting products from: European Foods\n",
      "      ✓ Extracted 179 products\n",
      "    [80/157] Extracting products from: Canned Goods\n",
      "      ✓ Extracted 17 products\n",
      "    [81/157] Extracting products from: Bakery & Deli\n",
      "      ✓ Extracted 29 products\n",
      "    [82/157] Extracting products from: Snacks, Cookies & Crackers\n",
      "      ✓ Extracted 18 products\n",
      "    [83/157] Extracting products from: Shop More European Foods\n",
      "      ✓ Extracted 179 products\n",
      "    [84/157] Extracting products from: Kosher Foods\n",
      "      ✓ Extracted 109 products\n",
      "    [85/157] Extracting products from: Snacks, Cookies & Crackers\n",
      "      ✓ Extracted 66 products\n",
      "    [86/157] Extracting products from: Baking, Breakfast & Dressings\n",
      "      ✓ Extracted 23 products\n",
      "    [87/157] Extracting products from: Meat & Seafood\n",
      "      ✓ Extracted 0 products\n",
      "\n",
      "  Processing subcategory 6/15: Snacks, Chips & Candy (5 subcategory2 items)\n",
      "    [88/157] Extracting products from: Chips & Snacks\n",
      "      ✓ Extracted 1076 products\n",
      "    [89/157] Extracting products from: Candy & Chocolate\n",
      "      ✓ Extracted 809 products\n",
      "    [90/157] Extracting products from: Crackers & Cookies\n",
      "      ✓ Extracted 639 products\n",
      "    [91/157] Extracting products from: Snack Cakes\n",
      "      ✓ Extracted 17 products\n",
      "    [92/157] Extracting products from: Bulk Nuts and Candy\n",
      "      ✓ Extracted 262 products\n",
      "\n",
      "  Processing subcategory 7/15: Frozen Food (9 subcategory2 items)\n",
      "    [93/157] Extracting products from: Ice Cream & Desserts\n",
      "      ✓ Extracted 454 products\n",
      "    [94/157] Extracting products from: Frozen Fruit & Vegetables\n",
      "      ✓ Extracted 147 products\n",
      "    [95/157] Extracting products from: Frozen Meat and Seafood\n",
      "      ✓ Extracted 326 products\n",
      "    [96/157] Extracting products from: Frozen Meals, Entrees & Sides\n",
      "      ✓ Extracted 384 products\n",
      "    [97/157] Extracting products from: Frozen Pizza\n",
      "      ✓ Extracted 100 products\n",
      "    [98/157] Extracting products from: Frozen Appetizers & Snacks\n",
      "      ✓ Extracted 83 products\n",
      "    [99/157] Extracting products from: Frozen Bakery & Breakfast\n",
      "      ✓ Extracted 112 products\n",
      "    [100/157] Extracting products from: Frozen Beverages & Ice\n",
      "      ✓ Extracted 13 products\n",
      "    [101/157] Extracting products from: Frozen Meatless Alternatives\n",
      "      ✓ Extracted 10 products\n",
      "\n",
      "  Processing subcategory 8/15: Natural and Organic (14 subcategory2 items)\n",
      "    [102/157] Extracting products from: Cereals, Spreads & Syrups\n",
      "      ✓ Extracted 214 products\n",
      "    [103/157] Extracting products from: Bakery\n",
      "      ✓ Extracted 32 products\n",
      "    [104/157] Extracting products from: Condiments, Sauces and Oils\n",
      "      ✓ Extracted 98 products\n",
      "    [105/157] Extracting products from: Snacks, Chips & Candy\n",
      "      ✓ Extracted 489 products\n",
      "    [106/157] Extracting products from: Dairy and Eggs\n",
      "      ✓ Extracted 256 products\n",
      "    [107/157] Extracting products from: Drinks\n",
      "      ✓ Extracted 317 products\n",
      "    [108/157] Extracting products from: Frozen Foods\n",
      "      ✓ Extracted 155 products\n",
      "    [109/157] Extracting products from: Baking and Spices\n",
      "      ✓ Extracted 204 products\n",
      "    [110/157] Extracting products from: Canned\n",
      "      ✓ Extracted 44 products\n",
      "    [111/157] Extracting products from: Pasta and Side Dishes\n",
      "      ✓ Extracted 208 products\n",
      "    [112/157] Extracting products from: Bars and Protein\n",
      "      ✓ Extracted 326 products\n",
      "    [113/157] Extracting products from: Health & Beauty\n",
      "      ✓ Extracted 242 products\n",
      "    [114/157] Extracting products from: Household Supplies\n",
      "      ✓ Extracted 46 products\n",
      "    [115/157] Extracting products from: Vitamins, Minerals and Supplements\n",
      "      ✓ Extracted 413 products\n",
      "\n",
      "  Processing subcategory 9/15: Bakery (6 subcategory2 items)\n",
      "    [116/157] Extracting products from: Bread\n",
      "      ✓ Extracted 219 products\n",
      "    [117/157] Extracting products from: Buns & Rolls\n",
      "      ✓ Extracted 68 products\n",
      "    [118/157] Extracting products from: Cookies, Muffins & Desserts\n",
      "      ✓ Extracted 186 products\n",
      "    [119/157] Extracting products from: Bagels, Croissants & English Muffins\n",
      "      ✓ Extracted 53 products\n",
      "    [120/157] Extracting products from: Wraps, Flatbread & Pizza Crust\n",
      "      ✓ Extracted 59 products\n",
      "    [121/157] Extracting products from: Cakes\n",
      "      ✓ Extracted 146 products\n",
      "\n",
      "  Processing subcategory 10/15: Prepared Meals (10 subcategory2 items)\n",
      "    [122/157] Extracting products from: Rotisserie & Fried Chicken\n",
      "      ✓ Extracted 36 products\n",
      "    [123/157] Extracting products from: Fresh Pasta & Sauce\n",
      "      ✓ Extracted 37 products\n",
      "    [124/157] Extracting products from: Entrees & Appetizers\n",
      "      ✓ Extracted 80 products\n",
      "    [125/157] Extracting products from: Salads & Soups\n",
      "      ✓ Extracted 61 products\n",
      "    [126/157] Extracting products from: Sandwiches\n",
      "      ✓ Extracted 36 products\n",
      "    [127/157] Extracting products from: Sushi\n",
      "      ✓ Extracted 45 products\n",
      "    [128/157] Extracting products from: Fries & Sides\n",
      "      ✓ Extracted 11 products\n",
      "    [129/157] Extracting products from: Pizza\n",
      "      ✓ Extracted 10 products\n",
      "    [130/157] Extracting products from: Quiches & Pies\n",
      "      ✓ Extracted 13 products\n",
      "    [131/157] Extracting products from: Snacks & Dips\n",
      "      ✓ Extracted 12 products\n",
      "\n",
      "  Processing subcategory 11/15: Drinks (9 subcategory2 items)\n",
      "    [132/157] Extracting products from: Juice\n",
      "      ✓ Extracted 330 products\n",
      "    [133/157] Extracting products from: Coffee\n",
      "      ✓ Extracted 308 products\n",
      "    [134/157] Extracting products from: Tea & Hot Drinks\n",
      "      ✓ Extracted 313 products\n",
      "    [135/157] Extracting products from: Soft Drinks\n",
      "      ✓ Extracted 181 products\n",
      "    [136/157] Extracting products from: Water\n",
      "      ✓ Extracted 210 products\n",
      "    [137/157] Extracting products from: Soy, Rice & Almond Drinks\n",
      "      ✓ Extracted 49 products\n",
      "    [138/157] Extracting products from: Sports & Energy\n",
      "      ✓ Extracted 158 products\n",
      "    [139/157] Extracting products from: Drink Mixes\n",
      "      ✓ Extracted 130 products\n",
      "    [140/157] Extracting products from: Non-alcoholic drinks\n",
      "      ✓ Extracted 75 products\n",
      "\n",
      "  Processing subcategory 12/15: Deli (7 subcategory2 items)\n",
      "    [141/157] Extracting products from: Deli Meat\n",
      "      ✓ Extracted 217 products\n",
      "    [142/157] Extracting products from: Deli Cheese\n",
      "      ✓ Extracted 359 products\n",
      "    [143/157] Extracting products from: Antipasto, Dips & Spreads\n",
      "      ✓ Extracted 89 products\n",
      "    [144/157] Extracting products from: Crackers & Condiments\n",
      "      ✓ Extracted 192 products\n",
      "    [145/157] Extracting products from: Vegan & Vegetarian\n",
      "      ✓ Extracted 56 products\n",
      "    [146/157] Extracting products from: Lunch & Snack Kits\n",
      "      ✓ Extracted 23 products\n",
      "    [147/157] Extracting products from: Party Trays\n",
      "      ✓ Extracted 21 products\n",
      "\n",
      "  Processing subcategory 13/15: Fish & Seafood (7 subcategory2 items)\n",
      "    [148/157] Extracting products from: Shrimp\n",
      "      ✓ Extracted 36 products\n",
      "    [149/157] Extracting products from: Salmon\n",
      "      ✓ Extracted 30 products\n",
      "    [150/157] Extracting products from: Fish\n",
      "      ✓ Extracted 84 products\n",
      "    [151/157] Extracting products from: Smoked Fish\n",
      "      ✓ Extracted 20 products\n",
      "    [152/157] Extracting products from: Seafood Appetizers\n",
      "      ✓ Extracted 49 products\n",
      "    [153/157] Extracting products from: Shellfish\n",
      "      ✓ Extracted 61 products\n",
      "    [154/157] Extracting products from: Squid & Octopus\n",
      "      ✓ Extracted 8 products\n",
      "\n",
      "  Processing subcategory 14/15: Beer & Wine (3 subcategory2 items)\n",
      "    [155/157] Extracting products from: Beer\n",
      "      ✓ Extracted 78 products\n",
      "    [156/157] Extracting products from: Wine\n",
      "      ✓ Extracted 0 products\n",
      "    [157/157] Extracting products from: Coolers & Ciders\n",
      "      ✓ Extracted 21 products\n",
      "\n",
      "======================================================================\n",
      "PROCESSING CATEGORY 2/2: HOME, BEAUTY & BABY\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "EXPORTING DATA TO JSON\n",
      "======================================================================\n",
      "✓ Data exported to: loblaws_products.json\n",
      "✓ Total products: 23194\n",
      "\n",
      "======================================================================\n",
      "EXTRACTION SUMMARY\n",
      "======================================================================\n",
      "Categories processed: 2\n",
      "Subcategories processed: 15\n",
      "Subcategory2 processed: 157\n",
      "Total products extracted: 23194\n",
      "Errors encountered: 1\n",
      "\n",
      "⚠ Errors encountered:\n",
      "  1. Error extracting subcategories for 'HOME, BEAUTY & BABY': Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"div[data-testid=\"iceberg-main-nav-l2-tabs\"]\"}\n",
      "  (Session info: chrome=143.0.7499.41); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#nosuchelementexception\n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0x7ff6c30f8245\n",
      "\t0x7ff6c30f82a0\n",
      "\t0x7ff6c2ed165d\n",
      "\t0x7ff6c2f29a33\n",
      "\t0x7ff6c2f29d3c\n",
      "\t0x7ff6c2f1c88c\n",
      "\t0x7ff6c2f1c746\n",
      "\t0x7ff6c2f7ac97\n",
      "\t0x7ff6c2f1ac29\n",
      "\t0x7ff6c2f1ba93\n",
      "\t0x7ff6c340ffe0\n",
      "\t0x7ff6c340a920\n",
      "\t0x7ff6c3429086\n",
      "\t0x7ff6c3115744\n",
      "\t0x7ff6c311e6ec\n",
      "\t0x7ff6c3101964\n",
      "\t0x7ff6c3101b15\n",
      "\t0x7ff6c30e7842\n",
      "\t0x7ff81b6ae8d7\n",
      "\t0x7ff81d86c53c\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PIPELINE COMPLETED SUCCESSFULLY\n",
      "======================================================================\n",
      "\n",
      "✓ Pipeline execution completed!\n",
      "\n",
      "✓ Driver closed\n"
     ]
    }
   ],
   "source": [
    "### Run the pipeline\n",
    "\n",
    "# Initialize driver and wait\n",
    "driver = get_driver(headless=False)\n",
    "wait = WebDriverWait(driver, MAX_WAIT)\n",
    "\n",
    "try:\n",
    "    # Run the pipeline\n",
    "    stats = extract_all_products_pipeline(\n",
    "        driver, \n",
    "        wait, \n",
    "        output_file=\"loblaws_products.json\",\n",
    "        debug=False  # Set to True for detailed debug output\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Pipeline execution completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Pipeline execution failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    driver.quit()\n",
    "    print(\"\\n✓ Driver closed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701cd3a5",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# GROUND TRUTH VALIDATION - Structured by Category Hierarchy\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_ground_truth_by_hierarchy(extracted_file, output_file=\"ground_truth.json\", products_per_subcategory2=10):\n",
    "    \"\"\"\n",
    "    Create ground truth data with specified number of products per subcategory2.\n",
    "    Products are grouped by category > subcategory > subcategory2.\n",
    "    All products in each group have the same category, subcategory, and subcategory2.\n",
    "    \n",
    "    Args:\n",
    "        extracted_file: Path to extracted products JSON file\n",
    "        output_file: Path to output ground truth file\n",
    "        products_per_subcategory2: Number of products to select per subcategory2 (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Ground truth data structure\n",
    "    \"\"\"\n",
    "    # Load extracted products\n",
    "    with open(extracted_file, 'r', encoding='utf-8') as f:\n",
    "        extracted_data = json.load(f)\n",
    "    \n",
    "    all_products = extracted_data.get('products', [])\n",
    "    \n",
    "    if not all_products:\n",
    "        print(\"✗ No products found in extraction file\")\n",
    "        return None\n",
    "    \n",
    "    # Group products by category > subcategory > subcategory2\n",
    "    grouped_products = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    \n",
    "    for product in all_products:\n",
    "        category = product.get('category', '').strip()\n",
    "        subcategory = product.get('subcategory', '').strip()\n",
    "        subcategory2 = product.get('subcategory2', '').strip()\n",
    "        \n",
    "        if category and subcategory and subcategory2:\n",
    "            grouped_products[category][subcategory][subcategory2].append(product)\n",
    "    \n",
    "    # Create ground truth structure\n",
    "    ground_truth = {\n",
    "        \"version\": \"1.0\",\n",
    "        \"created_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"description\": f\"Ground truth data with {products_per_subcategory2} products per subcategory2\",\n",
    "        \"products_per_subcategory2\": products_per_subcategory2,\n",
    "        \"verification_status\": \"pending\",\n",
    "        \"statistics\": {\n",
    "            \"total_categories\": 0,\n",
    "            \"total_subcategories\": 0,\n",
    "            \"total_subcategory2\": 0,\n",
    "            \"total_products\": 0\n",
    "        },\n",
    "        \"categories\": []\n",
    "    }\n",
    "    \n",
    "    # Process each category\n",
    "    for category_name in sorted(grouped_products.keys()):\n",
    "        category_data = {\n",
    "            \"category\": category_name,\n",
    "            \"subcategories\": []\n",
    "        }\n",
    "        \n",
    "        # Process each subcategory\n",
    "        for subcategory_name in sorted(grouped_products[category_name].keys()):\n",
    "            subcategory_data = {\n",
    "                \"subcategory\": subcategory_name,\n",
    "                \"subcategory2_items\": []\n",
    "            }\n",
    "            \n",
    "            # Process each subcategory2\n",
    "            for subcategory2_name in sorted(grouped_products[category_name][subcategory_name].keys()):\n",
    "                products_in_subcat2 = grouped_products[category_name][subcategory_name][subcategory2_name]\n",
    "                \n",
    "                # Select up to N products (or all if less than N)\n",
    "                num_to_select = min(products_per_subcategory2, len(products_in_subcat2))\n",
    "                selected_products = products_in_subcat2[:num_to_select]  # Take first N, or use random.sample() for random selection\n",
    "                \n",
    "                subcategory2_data = {\n",
    "                    \"subcategory2\": subcategory2_name,\n",
    "                    \"expected_count\": products_per_subcategory2,\n",
    "                    \"actual_count\": num_to_select,\n",
    "                    \"products\": []\n",
    "                }\n",
    "                \n",
    "                # Add selected products\n",
    "                for product in selected_products:\n",
    "                    gt_product = {\n",
    "                        \"product-url\": product.get(\"product-url\", \"\"),\n",
    "                        \"product-title\": product.get(\"product-title\", \"\"),\n",
    "                        \"regular-price\": product.get(\"regular-price\", \"\"),\n",
    "                        \"product-package-size\": product.get(\"product-package-size\", \"\"),\n",
    "                        \"category\": product.get(\"category\", \"\"),\n",
    "                        \"subcategory\": product.get(\"subcategory\", \"\"),\n",
    "                        \"subcategory2\": product.get(\"subcategory2\", \"\"),\n",
    "                        \"verified\": False,\n",
    "                        \"is_correct\": None,\n",
    "                        \"notes\": \"\"\n",
    "                    }\n",
    "                    subcategory2_data[\"products\"].append(gt_product)\n",
    "                \n",
    "                subcategory_data[\"subcategory2_items\"].append(subcategory2_data)\n",
    "                ground_truth[\"statistics\"][\"total_subcategory2\"] += 1\n",
    "                ground_truth[\"statistics\"][\"total_products\"] += len(subcategory2_data[\"products\"])\n",
    "            \n",
    "            category_data[\"subcategories\"].append(subcategory_data)\n",
    "            ground_truth[\"statistics\"][\"total_subcategories\"] += len(subcategory_data[\"subcategory2_items\"])\n",
    "        \n",
    "        ground_truth[\"categories\"].append(category_data)\n",
    "        ground_truth[\"statistics\"][\"total_categories\"] += 1\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(ground_truth, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"=\" * 70)\n",
    "    print(\"GROUND TRUTH CREATED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"✓ Output file: {output_file}\")\n",
    "    print(f\"✓ Products per subcategory2: {products_per_subcategory2}\")\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Total Categories: {ground_truth['statistics']['total_categories']}\")\n",
    "    print(f\"  Total Subcategories: {ground_truth['statistics']['total_subcategories']}\")\n",
    "    print(f\"  Total Subcategory2: {ground_truth['statistics']['total_subcategory2']}\")\n",
    "    print(f\"  Total Products: {ground_truth['statistics']['total_products']}\")\n",
    "    \n",
    "    print(f\"\\nBreakdown by Category:\")\n",
    "    for cat_data in ground_truth[\"categories\"]:\n",
    "        cat_name = cat_data[\"category\"]\n",
    "        total_subcat2 = sum(len(subcat[\"subcategory2_items\"]) for subcat in cat_data[\"subcategories\"])\n",
    "        total_products = sum(\n",
    "            len(subcat2[\"products\"]) \n",
    "            for subcat in cat_data[\"subcategories\"] \n",
    "            for subcat2 in subcat[\"subcategory2_items\"]\n",
    "        )\n",
    "        print(f\"  {cat_name}: {total_subcat2} subcategory2 items, {total_products} products\")\n",
    "    \n",
    "    print(f\"\\n⚠ Please manually verify each product:\")\n",
    "    print(f\"  1. Open the product URL in a browser\")\n",
    "    print(f\"  2. Verify each field (title, price, size, category hierarchy)\")\n",
    "    print(f\"  3. Set 'verified': true and 'is_correct': true/false\")\n",
    "    print(f\"  4. Add any notes in the 'notes' field\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return ground_truth\n",
    "\n",
    "\n",
    "def create_random_sample_for_verification(extracted_file, output_file=\"random_sample_verification.json\", num_products=50):\n",
    "    \"\"\"\n",
    "    Create a random sample of products from extraction for manual verification.\n",
    "    \n",
    "    Args:\n",
    "        extracted_file: Path to extracted products JSON file\n",
    "        output_file: Path to output sample file\n",
    "        num_products: Number of products to randomly sample (default: 50)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Random sample data\n",
    "    \"\"\"\n",
    "    # Load extracted products\n",
    "    with open(extracted_file, 'r', encoding='utf-8') as f:\n",
    "        extracted_data = json.load(f)\n",
    "    \n",
    "    products = extracted_data.get('products', [])\n",
    "    \n",
    "    if len(products) < num_products:\n",
    "        print(f\"⚠ Warning: Only {len(products)} products available, using all of them\")\n",
    "        num_products = len(products)\n",
    "    \n",
    "    # Randomly sample products\n",
    "    random.seed()  # Use current time as seed\n",
    "    selected_products = random.sample(products, num_products)\n",
    "    \n",
    "    # Create sample structure\n",
    "    sample_data = {\n",
    "        \"version\": \"1.0\",\n",
    "        \"created_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"description\": f\"Random sample of {num_products} products for manual verification\",\n",
    "        \"total_products_in_extraction\": len(products),\n",
    "        \"sample_size\": num_products,\n",
    "        \"verification_status\": \"pending\",\n",
    "        \"products\": []\n",
    "    }\n",
    "    \n",
    "    # Add products with indices for reference\n",
    "    for idx, product in enumerate(selected_products, 1):\n",
    "        sample_product = {\n",
    "            \"sample_index\": idx,\n",
    "            \"product-url\": product.get(\"product-url\", \"\"),\n",
    "            \"product-title\": product.get(\"product-title\", \"\"),\n",
    "            \"regular-price\": product.get(\"regular-price\", \"\"),\n",
    "            \"product-package-size\": product.get(\"product-package-size\", \"\"),\n",
    "            \"category\": product.get(\"category\", \"\"),\n",
    "            \"subcategory\": product.get(\"subcategory\", \"\"),\n",
    "            \"subcategory2\": product.get(\"subcategory2\", \"\"),\n",
    "            \"verified\": False,\n",
    "            \"is_correct\": None,\n",
    "            \"notes\": \"\"\n",
    "        }\n",
    "        sample_data[\"products\"].append(sample_product)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sample_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✓ Random sample created: {output_file}\")\n",
    "    print(f\"✓ Contains {len(sample_data['products'])} randomly selected products\")\n",
    "    print(f\"\\n⚠ Please manually verify each product and update the 'verified' and 'is_correct' fields\")\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "\n",
    "def compare_with_ground_truth(extracted_file, ground_truth_file):\n",
    "    \"\"\"\n",
    "    Compare extracted products against ground truth.\n",
    "    Works with the hierarchical ground truth structure.\n",
    "    \n",
    "    Args:\n",
    "        extracted_file: Path to extracted products JSON file\n",
    "        ground_truth_file: Path to ground truth JSON file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comparison results\n",
    "    \"\"\"\n",
    "    # Load files\n",
    "    with open(extracted_file, 'r', encoding='utf-8') as f:\n",
    "        extracted_data = json.load(f)\n",
    "    \n",
    "    with open(ground_truth_file, 'r', encoding='utf-8') as f:\n",
    "        ground_truth_data = json.load(f)\n",
    "    \n",
    "    extracted_products = extracted_data.get('products', [])\n",
    "    \n",
    "    # Extract all ground truth products from hierarchical structure\n",
    "    gt_products = []\n",
    "    for category_data in ground_truth_data.get('categories', []):\n",
    "        for subcategory_data in category_data.get('subcategories', []):\n",
    "            for subcategory2_data in subcategory_data.get('subcategory2_items', []):\n",
    "                for product in subcategory2_data.get('products', []):\n",
    "                    gt_products.append(product)\n",
    "    \n",
    "    # Create a lookup dictionary by URL (most reliable identifier)\n",
    "    extracted_by_url = {p.get('product-url', ''): p for p in extracted_products if p.get('product-url')}\n",
    "    \n",
    "    results = {\n",
    "        \"comparison_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"total_ground_truth\": len(gt_products),\n",
    "        \"total_extracted\": len(extracted_products),\n",
    "        \"matched_by_url\": 0,\n",
    "        \"field_comparisons\": {\n",
    "            \"product-title\": {\"exact_match\": 0, \"mismatch\": 0, \"missing_in_extracted\": 0, \"missing_in_ground_truth\": 0},\n",
    "            \"regular-price\": {\"exact_match\": 0, \"mismatch\": 0, \"missing_in_extracted\": 0, \"missing_in_ground_truth\": 0},\n",
    "            \"product-package-size\": {\"exact_match\": 0, \"mismatch\": 0, \"missing_in_extracted\": 0, \"missing_in_ground_truth\": 0},\n",
    "            \"category\": {\"exact_match\": 0, \"mismatch\": 0, \"missing_in_extracted\": 0, \"missing_in_ground_truth\": 0},\n",
    "            \"subcategory\": {\"exact_match\": 0, \"mismatch\": 0, \"missing_in_extracted\": 0, \"missing_in_ground_truth\": 0},\n",
    "            \"subcategory2\": {\"exact_match\": 0, \"mismatch\": 0, \"missing_in_extracted\": 0, \"missing_in_ground_truth\": 0}\n",
    "        },\n",
    "        \"detailed_comparisons\": [],\n",
    "        \"not_found_in_extraction\": [],\n",
    "        \"by_category\": defaultdict(lambda: {\n",
    "            \"total\": 0,\n",
    "            \"matched\": 0,\n",
    "            \"field_accuracy\": defaultdict(lambda: {\"exact_match\": 0, \"mismatch\": 0, \"total\": 0})\n",
    "        })\n",
    "    }\n",
    "    \n",
    "    # Compare each ground truth product\n",
    "    for gt_product in gt_products:\n",
    "        gt_url = gt_product.get('product-url', '')\n",
    "        gt_category = gt_product.get('category', '')\n",
    "        \n",
    "        results[\"by_category\"][gt_category][\"total\"] += 1\n",
    "        \n",
    "        if gt_url in extracted_by_url:\n",
    "            results[\"matched_by_url\"] += 1\n",
    "            results[\"by_category\"][gt_category][\"matched\"] += 1\n",
    "            extracted_product = extracted_by_url[gt_url]\n",
    "            \n",
    "            comparison = {\n",
    "                \"product-url\": gt_url,\n",
    "                \"product-title\": gt_product.get('product-title', ''),\n",
    "                \"category\": gt_category,\n",
    "                \"fields\": {}\n",
    "            }\n",
    "            \n",
    "            # Compare each field\n",
    "            for field in [\"product-title\", \"regular-price\", \"product-package-size\", \n",
    "                         \"category\", \"subcategory\", \"subcategory2\"]:\n",
    "                gt_value = gt_product.get(field, '').strip()\n",
    "                ext_value = extracted_product.get(field, '').strip()\n",
    "                \n",
    "                if not gt_value and not ext_value:\n",
    "                    status = \"both_missing\"\n",
    "                elif not ext_value:\n",
    "                    status = \"missing_in_extracted\"\n",
    "                    results[\"field_comparisons\"][field][\"missing_in_extracted\"] += 1\n",
    "                elif not gt_value:\n",
    "                    status = \"missing_in_ground_truth\"\n",
    "                    results[\"field_comparisons\"][field][\"missing_in_ground_truth\"] += 1\n",
    "                elif gt_value.lower() == ext_value.lower():\n",
    "                    status = \"exact_match\"\n",
    "                    results[\"field_comparisons\"][field][\"exact_match\"] += 1\n",
    "                    results[\"by_category\"][gt_category][\"field_accuracy\"][field][\"exact_match\"] += 1\n",
    "                else:\n",
    "                    status = \"mismatch\"\n",
    "                    results[\"field_comparisons\"][field][\"mismatch\"] += 1\n",
    "                    results[\"by_category\"][gt_category][\"field_accuracy\"][field][\"mismatch\"] += 1\n",
    "                \n",
    "                results[\"by_category\"][gt_category][\"field_accuracy\"][field][\"total\"] += 1\n",
    "                \n",
    "                comparison[\"fields\"][field] = {\n",
    "                    \"ground_truth\": gt_value,\n",
    "                    \"extracted\": ext_value,\n",
    "                    \"status\": status\n",
    "                }\n",
    "            \n",
    "            results[\"detailed_comparisons\"].append(comparison)\n",
    "        else:\n",
    "            # Product not found in extraction\n",
    "            results[\"not_found_in_extraction\"].append({\n",
    "                \"product-url\": gt_url,\n",
    "                \"product-title\": gt_product.get('product-title', ''),\n",
    "                \"category\": gt_category\n",
    "            })\n",
    "    \n",
    "    # Calculate accuracy percentages\n",
    "    total_gt = len(gt_products)\n",
    "    if total_gt > 0:\n",
    "        results[\"match_rate\"] = (results[\"matched_by_url\"] / total_gt) * 100\n",
    "        \n",
    "        results[\"field_accuracy\"] = {}\n",
    "        for field, stats in results[\"field_comparisons\"].items():\n",
    "            total = sum(stats.values())\n",
    "            if total > 0:\n",
    "                exact_pct = (stats[\"exact_match\"] / total) * 100\n",
    "                results[\"field_accuracy\"][field] = {\n",
    "                    \"exact_match_percentage\": f\"{exact_pct:.2f}%\",\n",
    "                    \"exact_matches\": stats[\"exact_match\"],\n",
    "                    \"mismatches\": stats[\"mismatch\"],\n",
    "                    \"missing_in_extracted\": stats[\"missing_in_extracted\"]\n",
    "                }\n",
    "        \n",
    "        # Calculate per-category accuracy\n",
    "        for category, cat_stats in results[\"by_category\"].items():\n",
    "            if cat_stats[\"total\"] > 0:\n",
    "                cat_stats[\"match_rate\"] = (cat_stats[\"matched\"] / cat_stats[\"total\"]) * 100\n",
    "                cat_stats[\"field_accuracy_percentages\"] = {}\n",
    "                for field, field_stats in cat_stats[\"field_accuracy\"].items():\n",
    "                    if field_stats[\"total\"] > 0:\n",
    "                        exact_pct = (field_stats[\"exact_match\"] / field_stats[\"total\"]) * 100\n",
    "                        cat_stats[\"field_accuracy_percentages\"][field] = f\"{exact_pct:.2f}%\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_comparison_report(comparison_results):\n",
    "    \"\"\"Print a formatted comparison report.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"GROUND TRUTH COMPARISON REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"  Total Ground Truth Products: {comparison_results['total_ground_truth']}\")\n",
    "    print(f\"  Total Extracted Products: {comparison_results['total_extracted']}\")\n",
    "    print(f\"  Matched by URL: {comparison_results['matched_by_url']}\")\n",
    "    if comparison_results.get('match_rate'):\n",
    "        print(f\"  Match Rate: {comparison_results['match_rate']:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nField-Level Accuracy (Overall):\")\n",
    "    if comparison_results.get('field_accuracy'):\n",
    "        for field, accuracy in comparison_results['field_accuracy'].items():\n",
    "            print(f\"  {field}:\")\n",
    "            print(f\"    Exact Match: {accuracy['exact_match_percentage']} ({accuracy['exact_matches']} matches)\")\n",
    "            print(f\"    Mismatches: {accuracy['mismatches']}\")\n",
    "            print(f\"    Missing in Extraction: {accuracy['missing_in_extracted']}\")\n",
    "    \n",
    "    # Per-category breakdown\n",
    "    if comparison_results.get('by_category'):\n",
    "        print(f\"\\nAccuracy by Category:\")\n",
    "        for category, cat_stats in comparison_results['by_category'].items():\n",
    "            if cat_stats.get('total', 0) > 0:\n",
    "                print(f\"  {category}:\")\n",
    "                print(f\"    Match Rate: {cat_stats.get('match_rate', 0):.2f}% ({cat_stats['matched']}/{cat_stats['total']})\")\n",
    "                if cat_stats.get('field_accuracy_percentages'):\n",
    "                    print(f\"    Field Accuracy:\")\n",
    "                    for field, pct in cat_stats['field_accuracy_percentages'].items():\n",
    "                        print(f\"      {field}: {pct}\")\n",
    "    \n",
    "    if comparison_results['not_found_in_extraction']:\n",
    "        print(f\"\\n⚠ Products in Ground Truth but NOT found in Extraction: {len(comparison_results['not_found_in_extraction'])}\")\n",
    "        print(\"  First 5 missing products:\")\n",
    "        for i, missing in enumerate(comparison_results['not_found_in_extraction'][:5], 1):\n",
    "            print(f\"    {i}. {missing.get('product-title', 'N/A')}\")\n",
    "            print(f\"       Category: {missing.get('category', 'N/A')}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "\n",
    "def save_comparison_report(comparison_results, output_file=\"comparison_report.json\"):\n",
    "    \"\"\"Save comparison results to JSON file.\"\"\"\n",
    "    # Convert defaultdict to regular dict for JSON serialization\n",
    "    def convert_defaultdict(obj):\n",
    "        if isinstance(obj, defaultdict):\n",
    "            return {k: convert_defaultdict(v) for k, v in obj.items()}\n",
    "        return obj\n",
    "    \n",
    "    serializable_results = convert_defaultdict(comparison_results)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"✓ Comparison report saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556c09d",
   "metadata": {},
   "source": [
    "\n",
    "### Test validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167791e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ground truth with 10 products per subcategory2\n",
    "ground_truth = create_ground_truth_by_hierarchy(\n",
    "    extracted_file=\"loblaws_products.json\",\n",
    "    output_file=\"ground_truth.json\",\n",
    "    products_per_subcategory2=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random sample (50 products for random verification)\n",
    "random_sample = create_random_sample_for_verification(\n",
    "    extracted_file=\"loblaws_products.json\",\n",
    "    output_file=\"random_sample_verification.json\",\n",
    "    num_products=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After manually verifying ground_truth.json, compare results\n",
    "comparison_results = compare_with_ground_truth(\n",
    "    extracted_file=\"loblaws_products.json\",\n",
    "    ground_truth_file=\"ground_truth.json\"\n",
    ")\n",
    "\n",
    "# Print report\n",
    "print_comparison_report(comparison_results)\n",
    "\n",
    "# Save detailed report\n",
    "save_comparison_report(comparison_results, \"comparison_report.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb38ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
